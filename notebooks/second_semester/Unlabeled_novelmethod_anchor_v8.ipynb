{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Experiments to run:\n",
        "\n",
        "Note: Fix random seed numpy, pytorch (data splitting), log data to csv files, run each 5 times\n",
        "\n",
        "1. Dataset:\n",
        "  1.   Increase training size\n",
        "  2.   Decrease unlabeled data size\n",
        "\n",
        "2. Methods:\n",
        "  1.   Distillation with lambdas: 0., 0.3, 0.5, 0.7, 0.9\n",
        "  2.   Distillation with unlabeled with soft labels from teacher ../../\n",
        "  3.   Anchor method with similar lambdas\n",
        "  4.   Anchor method with unlabeled ../../\n",
        "\n",
        "3. Metrics:\n",
        "  1.   Good churn\n",
        "  2.   Bad churn\n",
        "  3.   Churn ratio\n",
        "  4.   Accuracy"
      ],
      "metadata": {
        "id": "wnRPSzCS6zV7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGI6GNXhYkFq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from typing import Union\n",
        "\n",
        "N_EPOCHS = 50"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for PyTorch\n",
        "torch.manual_seed(46)\n",
        "\n",
        "# Set the seed for NumPy\n",
        "np.random.seed(46)\n",
        "\n",
        "class ChurnMetric:\n",
        "    \"\"\"\n",
        "    Super class for metrics.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tensor_type=\"numpy\") -> None:\n",
        "        tensor_types = {\"numpy\": np.ndarray, \"torch\": torch.Tensor}\n",
        "        if tensor_type not in tensor_types:\n",
        "            raise NotImplementedError(\"Unknown object type\")\n",
        "        self.tensor_type = tensor_types[tensor_type]\n",
        "\n",
        "    def call_sanitize_inputs(self, **preds):\n",
        "        \"\"\"\n",
        "        preds can be true labels as well\n",
        "        Shape of tensors need to be the same.\n",
        "        Tensor dim len must not be >2\n",
        "        \"\"\"\n",
        "        for p in preds:\n",
        "            if not isinstance(preds[p], self.tensor_type):\n",
        "                raise TypeError(f\"{p} is not an instance of {str(self.tensor_type)}\")\n",
        "            if len(preds[p].shape) > 2:\n",
        "                raise ValueError(f\"Too many dims in {p}\")\n",
        "        shapes = set([preds[p].shape for p in preds])\n",
        "        if len(shapes) > 1:  # TODO extend to force only first dim to be the same\n",
        "            raise ValueError(f\"shape mismatch. shapes of preds must be same\")\n",
        "\n",
        "    def reshape_argmax(self, **preds):\n",
        "        for p in preds:\n",
        "            if len(preds[p].shape) == 2 and preds[p].shape[1] > 1:\n",
        "                preds[p] = preds[p].argmax(1)\n",
        "        return (preds[p] for p in preds)\n",
        "\n",
        "\n",
        "class Churn(ChurnMetric):\n",
        "    \"\"\"\n",
        "    Simple Churn. Calculates number of classification disagreements, along axis:0.\n",
        "    Will take argmax if multiple columns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tensor_type=\"numpy\", output_mode=\"proportion\") -> None:\n",
        "        super(Churn, self).__init__(tensor_type=tensor_type)\n",
        "        if output_mode not in {\"proportion\", \"count\"}:\n",
        "            raise ValueError(\"Unknown output_mode\")\n",
        "        self.output_mode = output_mode\n",
        "\n",
        "    def __call__(\n",
        "        self,\n",
        "        predA: Union[np.ndarray, torch.Tensor],\n",
        "        predB: Union[np.ndarray, torch.Tensor],\n",
        "    ) -> None:\n",
        "        self.call_sanitize_inputs(predA=predA, predB=predB)\n",
        "        predA, predB = self.reshape_argmax(predA=predA, predB=predB)\n",
        "\n",
        "        churn = sum(predA != predB)\n",
        "\n",
        "        if self.output_mode == \"proportion\":\n",
        "            return churn / predA.shape[0]\n",
        "        if self.output_mode == \"count\":\n",
        "            return churn\n",
        "\n",
        "\n",
        "class WinLossRatio(ChurnMetric):\n",
        "    \"\"\"\n",
        "    Lateral Churns are not loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, tensor_type=\"numpy\") -> None:\n",
        "        super().__init__(tensor_type)\n",
        "\n",
        "    def __call__(self, true_labels, pred_teacher, pred_student):\n",
        "        self.call_sanitize_inputs(\n",
        "            true_labels=true_labels,\n",
        "            pred_teacher=pred_teacher,\n",
        "            pred_student=pred_student,\n",
        "        )\n",
        "        true_labels, pred_teacher, pred_student = self.reshape_argmax(\n",
        "            true_labels, pred_teacher, pred_student\n",
        "        )\n",
        "\n",
        "        pred_teacher = pred_teacher == true_labels\n",
        "        pred_student = pred_student == true_labels\n",
        "        wins = sum(pred_student > pred_teacher)\n",
        "        losses = sum(pred_student < pred_teacher)\n",
        "\n",
        "        return wins / (losses + 1e-7)\n",
        "\n",
        "\n",
        "class ChurnRatio(ChurnMetric):\n",
        "    def __init__(self, tensor_type=\"numpy\") -> None:\n",
        "        super().__init__(tensor_type)\n",
        "\n",
        "    def __call__(self, pred_teacher, pred_student, pred_control):\n",
        "        self.call_sanitize_inputs(\n",
        "            pred_teacher=pred_teacher,\n",
        "            pred_student=pred_student,\n",
        "            pred_control=pred_control,\n",
        "        )\n",
        "        pred_teacher, pred_student, pred_control = self.reshape_argmax(\n",
        "            pred_teacher, pred_student, pred_control\n",
        "        )\n",
        "\n",
        "        churnratio = sum(pred_student != pred_teacher) / sum(\n",
        "            pred_control != pred_teacher\n",
        "        )\n",
        "        return churnratio\n",
        "\n",
        "\n",
        "class GoodBadChurn(ChurnMetric):\n",
        "    \"\"\"\n",
        "    lateral churn is bad\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, tensor_type=\"numpy\", mode=None, output_mode=\"proportion\"\n",
        "    ) -> None:\n",
        "        super().__init__(tensor_type)\n",
        "        if mode is None or mode not in {\"good\", \"bad\"}:\n",
        "            raise ValueError(\"Please specify mode as good or bad\")\n",
        "        self.mode = mode\n",
        "        if output_mode not in {\"proportion\", \"count\"}:\n",
        "            raise ValueError(\"Unknown output_mode\")\n",
        "        self.output_mode = output_mode\n",
        "\n",
        "    def __call__(self, true_labels, pred_teacher, pred_student):\n",
        "        self.call_sanitize_inputs(\n",
        "            true_labels=true_labels,\n",
        "            pred_teacher=pred_teacher,\n",
        "            pred_student=pred_student,\n",
        "        )\n",
        "        true_labels, pred_teacher, pred_student = self.reshape_argmax(\n",
        "            true_labels=true_labels, pred_teacher=pred_teacher, pred_student=pred_student,\n",
        "        )\n",
        "\n",
        "        if self.mode == \"good\":\n",
        "            churn = sum((pred_student == true_labels) > (pred_teacher == true_labels))\n",
        "            if self.output_mode == \"proportion\":\n",
        "                denominator = sum((pred_student == true_labels))\n",
        "                return churn / denominator\n",
        "            elif self.output_mode == \"count\":\n",
        "                return churn\n",
        "\n",
        "        elif self.mode == \"bad\":\n",
        "            churn = sum(\n",
        "                (pred_student == true_labels) < (pred_teacher == true_labels)\n",
        "            ) + sum(\n",
        "                (pred_student != true_labels)\n",
        "                & (pred_teacher != true_labels)\n",
        "                & (pred_teacher != pred_student)\n",
        "            )\n",
        "            if self.output_mode == \"proportion\":\n",
        "                raise NotImplementedError(\"IDK\")\n",
        "            elif self.output_mode == \"count\":\n",
        "                return churn\n"
      ],
      "metadata": {
        "id": "22GEbC3hxOie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init datasets"
      ],
      "metadata": {
        "id": "av1Qec0tq5WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the seed for PyTorch\n",
        "torch.manual_seed(46)\n",
        "\n",
        "# Set the seed for NumPy\n",
        "np.random.seed(46)\n",
        "\n",
        "#torch transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,)),\n",
        "])\n",
        "\n",
        "new_classes = [3,5,7]\n",
        "regular_classes = [0,1,2,4,6,8,9]\n",
        "\n",
        "#Create Train Sets\n",
        "total_datasets = [\n",
        "    torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform),\n",
        "    torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform),\n",
        "]\n",
        "\n",
        "total_dict = {\n",
        "    \"x\": torch.concat([d.data for d in total_datasets]),\n",
        "    \"y\": torch.concat([d.targets for d in total_datasets]),\n",
        "}\n",
        "total_dict[\"id\"] = np.array(range(total_dict[\"y\"].shape[0]))\n",
        "total_dict[\"field\"] = np.array(['NoneNoneNone']*total_dict['y'].shape[0])\n",
        "\n",
        "for c in regular_classes:\n",
        "    c_ids = total_dict[\"id\"][np.where(total_dict[\"y\"]==c)[0]]\n",
        "    field = np.array([\"oldtrain\"]*4400 + [\"test\"]*800 + [\"val\"]*800 + [\"unlab\"]*1000 + [\"new\"]*0)\n",
        "    np.random.shuffle(field)    \n",
        "    total_dict['field'][c_ids] = field\n",
        "for c in new_classes:\n",
        "    c_ids = total_dict[\"id\"][np.where(total_dict[\"y\"]==c)[0]]\n",
        "    field = np.array([\"oldtrain\"]*3900 + [\"test\"]*800 + [\"val\"]*800 + [\"unlab\"]*500 + [\"new\"]*1000)\n",
        "    np.random.shuffle(field)\n",
        "    total_dict['field'][c_ids] = field\n",
        "\n",
        "sets = {\n",
        "    ss: torch.utils.data.TensorDataset(\n",
        "        total_dict['x'][total_dict['field']==ss],\n",
        "        total_dict['y'][total_dict['field']==ss],\n",
        "    ) for ss in [\"oldtrain\", \"test\", \"val\", \"new\"]\n",
        "}\n",
        "sets[\"unlab\"] = torch.utils.data.TensorDataset(\n",
        "    total_dict['x'][total_dict['field']==\"unlab\"],\n",
        "    total_dict['y'][total_dict['field']==\"unlab\"],\n",
        ")\n",
        "sets['mixed'] = torch.utils.data.TensorDataset(\n",
        "    torch.concat([\n",
        "    total_dict['x'][total_dict['field']==\"oldtrain\"],\n",
        "    total_dict['x'][total_dict['field']==\"new\"],\n",
        "]),\n",
        "torch.concat([\n",
        "    total_dict['y'][total_dict['field']==\"oldtrain\"],\n",
        "    total_dict['y'][total_dict['field']==\"new\"],\n",
        "])\n",
        ")"
      ],
      "metadata": {
        "id": "5YzIzmxUr1Nc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model architecture"
      ],
      "metadata": {
        "id": "1tuEZA5aq7Lm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.features = nn.Sequential( #28*28\n",
        "#             nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1, padding=2, bias=True),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(6),\n",
        "#             nn.MaxPool2d(2),\n",
        "#             nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0, bias=True),\n",
        "#             nn.ReLU(),\n",
        "#             nn.BatchNorm2d(16),\n",
        "#             nn.MaxPool2d(2),\n",
        "#         )\n",
        "#         self.fully_connected = nn.Sequential(\n",
        "#             nn.Linear(in_features=16*5*5, out_features=120),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.25),\n",
        "#             nn.BatchNorm1d(120),\n",
        "#             nn.Linear(in_features=120, out_features=84),\n",
        "#             nn.ReLU(),\n",
        "#             nn.Dropout(0.25),\n",
        "#             nn.BatchNorm1d(84),\n",
        "#             nn.Linear(in_features=84, out_features=10),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.features(x)\n",
        "#         x = x.view(x.size(0), -1)\n",
        "#         x = self.fully_connected(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "D4K8BBWHyu0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout2d(0.25)\n",
        "        self.dropout2 = nn.Dropout2d(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "0nGeMLPLein3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple CNN"
      ],
      "metadata": {
        "id": "mqoK0XyqFNrD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CNN(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(CNN, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
        "#         self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "#         self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
        "#         self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "#         self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(torch.relu(self.conv1(x)))\n",
        "#         x = self.pool(torch.relu(self.conv2(x)))\n",
        "#         x = x.view(-1, 32 * 7 * 7)\n",
        "#         x = torch.relu(self.fc1(x))\n",
        "#         x = self.fc2(x)\n",
        "#         return x"
      ],
      "metadata": {
        "id": "Lg_mkvtNFEF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init models"
      ],
      "metadata": {
        "id": "DhVib-0Cr-Fn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 256\n",
        "loaders = {\n",
        "    ss: torch.utils.data.DataLoader(sets[ss], batch_size=batch_size, shuffle=False)\n",
        "    for ss in [\"oldtrain\", \"mixed\", \"test\", \"unlab\", \"val\", \"new\"]\n",
        "}"
      ],
      "metadata": {
        "id": "JW-TBKAw-5zZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Teacher"
      ],
      "metadata": {
        "id": "VueC8twrsXFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "\n",
        "def train(name, loader, init_val_loss=1e7, n_epochs=N_EPOCHS):\n",
        "    # Set the seed for PyTorch\n",
        "    torch.manual_seed(46)\n",
        "\n",
        "    # Set the seed for NumPy\n",
        "    np.random.seed(46)\n",
        "    model = CNN()\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    n_epochs = n_epochs\n",
        "    best_val_loss = init_val_loss\n",
        "    best_state_dict = deepcopy(model.state_dict())\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(n_epochs):\n",
        "        # running_loss = 0.0\n",
        "        for i, (x,y) in tqdm(enumerate(loader), disable=True,):\n",
        "            x = x.unsqueeze(1).float()\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, n_epochs, loss.item()))\n",
        "\n",
        "            \n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for i,data in enumerate(loader):\n",
        "                x, y = data\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                x = x.unsqueeze(1).float()\n",
        "                outputs = model(x)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += y.size(0)\n",
        "                correct += (predicted == y).sum().item()\n",
        "                val_loss += loss_fn(outputs, y).item()\n",
        "            if val_loss < best_val_loss:\n",
        "                model.to('cpu')\n",
        "                model.eval()\n",
        "                torch.save(model.state_dict(), name+\".pth\")\n",
        "                model.train()\n",
        "                model.to(device)\n",
        "                best_val_loss = val_loss\n",
        "                # print(f\"best at epoch {epoch}\")\n",
        "        # print('[%d] validation loss: %.5f' % (epoch+1, val_loss))\n",
        "                print('Accuracy of the model on the test images: {:.2f} %'.format(100 * correct / total))\n",
        "                print('Validation error of the model on the test images: {:.4f}'.format(val_loss / len(loader)))\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "BH1yVwv6Xt4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_dist(name, loader, init_val_loss=1e7, n_epochs=N_EPOCHS):\n",
        "    # Set the seed for PyTorch\n",
        "    torch.manual_seed(46)\n",
        "\n",
        "    # Set the seed for NumPy\n",
        "    np.random.seed(46)\n",
        "    model = CNN()\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    n_epochs = n_epochs\n",
        "    best_val_loss = init_val_loss\n",
        "    loss_fn = torch.nn.CrossEntropyLoss()\n",
        "    for epoch in range(n_epochs):\n",
        "        # running_loss = 0.0\n",
        "        for i, (x,y) in tqdm(enumerate(loader), disable=True,):\n",
        "            x = x.unsqueeze(1).float()\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = loss_fn(pred, y)\n",
        "            loss.backward()\n",
        "            optimizer.step() \n",
        "        print('Epoch [{}/{}], Loss: {:.4f}'\n",
        "                   .format(epoch+1, n_epochs, loss.item()))\n",
        "\n",
        "            \n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for i,data in enumerate(loader):\n",
        "                x, y = data\n",
        "                x = x.to(device)\n",
        "                y = y.to(device)\n",
        "                x = x.unsqueeze(1).float()\n",
        "                outputs = model(x)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                _, dist_y = torch.max(y.data, 1)\n",
        "                total += y.size(0)\n",
        "                correct += (predicted == dist_y).sum().item()\n",
        "                val_loss += loss_fn(outputs, y).item()\n",
        "            if val_loss < best_val_loss:\n",
        "                model.to('cpu')\n",
        "                model.eval()\n",
        "                torch.save(model.state_dict(), name+\".pth\")\n",
        "                model.train()\n",
        "                model.to(device)\n",
        "                best_val_loss = val_loss\n",
        "                # print(f\"best at epoch {epoch}\")\n",
        "        # print('[%d] validation loss: %.5f' % (epoch+1, val_loss))\n",
        "                print('Accuracy of the model on the test images: {:.2f} %'.format(100 * correct / total))\n",
        "                print('Validation error of the model on the test images: {:.4f}'.format(val_loss / len(loader)))\n",
        "    return model"
      ],
      "metadata": {
        "id": "jDfqy0XC5WB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Teacher Predicted Labels"
      ],
      "metadata": {
        "id": "TlO3sKHqsj10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    name=\"teacher_model\", \n",
        "    loader=loaders[\"oldtrain\"],\n",
        ")\n",
        "teacher_model = CNN()\n",
        "teacher_model.load_state_dict(torch.load(\"teacher_model.pth\"))\n",
        "teacher_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KF4LlAPGxnbB",
        "outputId": "9e78943a-cc71-474c-b306-e283b0091cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py:1331: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.1692\n",
            "Accuracy of the model on the test images: 81.05 %\n",
            "Validation error of the model on the test images: 0.5255\n",
            "Epoch [2/50], Loss: 0.3007\n",
            "Accuracy of the model on the test images: 83.75 %\n",
            "Validation error of the model on the test images: 0.4465\n",
            "Epoch [3/50], Loss: 0.1597\n",
            "Accuracy of the model on the test images: 85.65 %\n",
            "Validation error of the model on the test images: 0.3929\n",
            "Epoch [4/50], Loss: 0.0016\n",
            "Accuracy of the model on the test images: 86.65 %\n",
            "Validation error of the model on the test images: 0.3683\n",
            "Epoch [5/50], Loss: 0.0156\n",
            "Accuracy of the model on the test images: 87.00 %\n",
            "Validation error of the model on the test images: 0.3560\n",
            "Epoch [6/50], Loss: 0.0319\n",
            "Accuracy of the model on the test images: 87.29 %\n",
            "Validation error of the model on the test images: 0.3421\n",
            "Epoch [7/50], Loss: 0.0006\n",
            "Accuracy of the model on the test images: 88.41 %\n",
            "Validation error of the model on the test images: 0.3136\n",
            "Epoch [8/50], Loss: 0.0979\n",
            "Accuracy of the model on the test images: 88.62 %\n",
            "Validation error of the model on the test images: 0.3089\n",
            "Epoch [9/50], Loss: 0.0010\n",
            "Accuracy of the model on the test images: 89.08 %\n",
            "Validation error of the model on the test images: 0.2937\n",
            "Epoch [10/50], Loss: 0.0001\n",
            "Accuracy of the model on the test images: 89.28 %\n",
            "Validation error of the model on the test images: 0.2856\n",
            "Epoch [11/50], Loss: 0.0001\n",
            "Accuracy of the model on the test images: 89.58 %\n",
            "Validation error of the model on the test images: 0.2782\n",
            "Epoch [12/50], Loss: 0.0052\n",
            "Accuracy of the model on the test images: 90.00 %\n",
            "Validation error of the model on the test images: 0.2688\n",
            "Epoch [13/50], Loss: 0.0056\n",
            "Accuracy of the model on the test images: 90.06 %\n",
            "Validation error of the model on the test images: 0.2598\n",
            "Epoch [14/50], Loss: 0.0002\n",
            "Accuracy of the model on the test images: 90.33 %\n",
            "Validation error of the model on the test images: 0.2534\n",
            "Epoch [15/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 90.44 %\n",
            "Validation error of the model on the test images: 0.2490\n",
            "Epoch [16/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 91.26 %\n",
            "Validation error of the model on the test images: 0.2321\n",
            "Epoch [17/50], Loss: 0.0002\n",
            "Accuracy of the model on the test images: 91.29 %\n",
            "Validation error of the model on the test images: 0.2280\n",
            "Epoch [18/50], Loss: 0.0001\n",
            "Epoch [19/50], Loss: 0.0007\n",
            "Accuracy of the model on the test images: 91.61 %\n",
            "Validation error of the model on the test images: 0.2194\n",
            "Epoch [20/50], Loss: 0.0005\n",
            "Accuracy of the model on the test images: 91.73 %\n",
            "Validation error of the model on the test images: 0.2181\n",
            "Epoch [21/50], Loss: 0.0001\n",
            "Accuracy of the model on the test images: 91.98 %\n",
            "Validation error of the model on the test images: 0.2118\n",
            "Epoch [22/50], Loss: 0.0000\n",
            "Epoch [23/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 92.06 %\n",
            "Validation error of the model on the test images: 0.2036\n",
            "Epoch [24/50], Loss: 0.0002\n",
            "Accuracy of the model on the test images: 92.50 %\n",
            "Validation error of the model on the test images: 0.1969\n",
            "Epoch [25/50], Loss: 0.0010\n",
            "Accuracy of the model on the test images: 92.67 %\n",
            "Validation error of the model on the test images: 0.1922\n",
            "Epoch [26/50], Loss: 0.0000\n",
            "Epoch [27/50], Loss: 0.1369\n",
            "Accuracy of the model on the test images: 92.93 %\n",
            "Validation error of the model on the test images: 0.1868\n",
            "Epoch [28/50], Loss: 0.0004\n",
            "Accuracy of the model on the test images: 93.05 %\n",
            "Validation error of the model on the test images: 0.1821\n",
            "Epoch [29/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 93.31 %\n",
            "Validation error of the model on the test images: 0.1752\n",
            "Epoch [30/50], Loss: 0.0000\n",
            "Epoch [31/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 93.34 %\n",
            "Validation error of the model on the test images: 0.1724\n",
            "Epoch [32/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 93.33 %\n",
            "Validation error of the model on the test images: 0.1695\n",
            "Epoch [33/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 93.52 %\n",
            "Validation error of the model on the test images: 0.1688\n",
            "Epoch [34/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 93.64 %\n",
            "Validation error of the model on the test images: 0.1658\n",
            "Epoch [35/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 93.80 %\n",
            "Validation error of the model on the test images: 0.1629\n",
            "Epoch [36/50], Loss: 0.0157\n",
            "Accuracy of the model on the test images: 93.94 %\n",
            "Validation error of the model on the test images: 0.1603\n",
            "Epoch [37/50], Loss: 0.0001\n",
            "Accuracy of the model on the test images: 94.00 %\n",
            "Validation error of the model on the test images: 0.1559\n",
            "Epoch [38/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 94.09 %\n",
            "Validation error of the model on the test images: 0.1551\n",
            "Epoch [39/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 94.05 %\n",
            "Validation error of the model on the test images: 0.1536\n",
            "Epoch [40/50], Loss: 0.0002\n",
            "Accuracy of the model on the test images: 94.35 %\n",
            "Validation error of the model on the test images: 0.1458\n",
            "Epoch [41/50], Loss: 0.0000\n",
            "Epoch [42/50], Loss: 0.0030\n",
            "Accuracy of the model on the test images: 94.59 %\n",
            "Validation error of the model on the test images: 0.1407\n",
            "Epoch [43/50], Loss: 0.0007\n",
            "Epoch [44/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 94.52 %\n",
            "Validation error of the model on the test images: 0.1378\n",
            "Epoch [45/50], Loss: 0.0000\n",
            "Epoch [46/50], Loss: 0.0773\n",
            "Accuracy of the model on the test images: 94.69 %\n",
            "Validation error of the model on the test images: 0.1354\n",
            "Epoch [47/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 94.97 %\n",
            "Validation error of the model on the test images: 0.1300\n",
            "Epoch [48/50], Loss: 0.0000\n",
            "Accuracy of the model on the test images: 94.80 %\n",
            "Validation error of the model on the test images: 0.1299\n",
            "Epoch [49/50], Loss: 0.0001\n",
            "Epoch [50/50], Loss: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Get train labels on train mixed data\n",
        "with torch.no_grad():\n",
        "    teacher_labels_mixed = []\n",
        "    for (x,y) in tqdm(loaders['mixed']):\n",
        "        x = x.unsqueeze(1).float()\n",
        "        pred = teacher_model(x)\n",
        "        teacher_labels_mixed.append(pred)\n",
        "    teacher_labels_mixed = torch.concatenate(teacher_labels_mixed).argmax(dim=1)\n",
        "sets['teacher_mixed'] = torch.utils.data.TensorDataset(\n",
        "    torch.concat([\n",
        "    total_dict['x'][total_dict['field']==\"oldtrain\"],\n",
        "    total_dict['x'][total_dict['field']==\"new\"],\n",
        "]),\n",
        "    teacher_labels_mixed,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6k6LT28rsBT",
        "outputId": "03f40b60-f45a-4cb2-cda6-e662ad149dd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 178/178 [00:24<00:00,  7.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "# Get train labels on unlabelled data\n",
        "with torch.no_grad():\n",
        "    unlab_teacherpred = []\n",
        "    for (x,i) in tqdm(loaders['unlab']):\n",
        "        x = x.unsqueeze(1).float()\n",
        "        pred = teacher_model(x)\n",
        "        probs = F.softmax(pred, dim=1)\n",
        "        unlab_teacherpred.append(probs)\n",
        "    unlab_teacherpred = torch.cat(unlab_teacherpred, dim=0)  # Concatenate the list of tensors\n",
        "\n",
        "\n",
        "  # 0.1, 0.8, 0.1 : conf=0.8, pred=1\n",
        "  # 0.1, 0.3, 0.6 : conf=0.6, pred=2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr_HWSBX7uOL",
        "outputId": "8c568b30-b8d9-4f8a-848b-58d3b22f9eb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34/34 [00:06<00:00,  5.23it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unlab_teacherpred_conf, unlab_teacherpred = torch.concatenate(unlab_teacherpred).max(dim=1)"
      ],
      "metadata": {
        "id": "tfbfL7_XD4bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Misclassifications"
      ],
      "metadata": {
        "id": "hi_vEEQNTc8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "true_mix = torch.concat([\n",
        "    total_dict['y'][total_dict['field']==\"oldtrain\"],\n",
        "    total_dict['y'][total_dict['field']==\"new\"],\n",
        "])\n",
        "\n",
        "teacher_labels_mixed\n",
        "\n",
        "# Compare the tensors element-wise\n",
        "mask = true_mix != teacher_labels_mixed\n",
        "\n",
        "# Get the indices of the non-matching elements\n",
        "indices = torch.where(mask)\n"
      ],
      "metadata": {
        "id": "jVgKmA8Lgbi1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the unique elements and their counts\n",
        "unique, counts = torch.unique(true_mix[indices[0]], return_counts=True)\n",
        "\n",
        "# print the results\n",
        "for u, c in zip(unique, np.round(100*counts/len(true_mix),2)):\n",
        "    print('Element {}: {:.2f}%'.format(u, c))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9W9Fd6qUi3l",
        "outputId": "64dec314-6463-4166-a123-9b4a69cec407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Element 0: 0.51%\n",
            "Element 1: 0.01%\n",
            "Element 2: 0.41%\n",
            "Element 3: 0.27%\n",
            "Element 4: 0.18%\n",
            "Element 5: 0.04%\n",
            "Element 6: 0.44%\n",
            "Element 7: 0.07%\n",
            "Element 8: 0.01%\n",
            "Element 9: 0.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Adding Unlabeled Data"
      ],
      "metadata": {
        "id": "bIXAm-P5t3LW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_new = total_dict['x'][total_dict['field']==\"new\"].flatten(start_dim=1).numpy()\n",
        "X_mixed = torch.concat([\n",
        "    total_dict['x'][total_dict['field']==\"oldtrain\"],\n",
        "    total_dict['x'][total_dict['field']==\"new\"],\n",
        "]).flatten(start_dim=1).numpy()\n",
        "X_unlab = total_dict['x'][total_dict['field']==\"unlab\"].flatten(start_dim=1).numpy()\n",
        "y_mixed = torch.concat([\n",
        "    total_dict['y'][total_dict['field']==\"oldtrain\"],\n",
        "    total_dict['y'][total_dict['field']==\"new\"],\n",
        "])\n"
      ],
      "metadata": {
        "id": "Nhy76JTNF7g0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#relabel based on threshold\n",
        "confidence_threshold = 1.0\n",
        "\n",
        "# gmm_labels = teacher_labels_mixed\n",
        "lamda = 0.0\n",
        "gmm_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * (lamda) + F.one_hot(sets['mixed'].tensors[1], num_classes=10) * (1-lamda)\n",
        "#gmm_labels[loglik_mixed > threshold] = y_mixed[loglik_mixed > threshold]\n",
        "#unlabeled data\n",
        "# unlab_condition = torch.isin(unlab_teacherpred, torch.tensor(regular_classes))\n",
        "sets[\"teacher_unlab\"] = torch.utils.data.TensorDataset(\n",
        "    total_dict['x'][np.isin(total_dict['field'], [\"unlab\"])],\n",
        "    unlab_teacherpred\n",
        ")\n",
        "\n",
        "sets[\"gmm\"] = torch.utils.data.TensorDataset(\n",
        "    torch.concat([\n",
        "        sets['mixed'].tensors[0],\n",
        "        sets['unlab'].tensors[0],\n",
        "    ]),\n",
        "    torch.concat([\n",
        "        gmm_labels,\n",
        "       unlab_teacherpred\n",
        "    ]),\n",
        ")\n",
        "loaders['gmm'] = torch.utils.data.DataLoader(sets[\"gmm\"], batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "NdMMcSN-lhGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_new = total_dict['y'][total_dict['field']==\"new\"]"
      ],
      "metadata": {
        "id": "8qDau8tSGSTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_unlab = total_dict['y'][total_dict['field']==\"unlab\"].flatten(start_dim=0).numpy()"
      ],
      "metadata": {
        "id": "wkofeWg9rBVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train unlabel based CNN model"
      ],
      "metadata": {
        "id": "rHMEszCCtkBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dist(\n",
        "    name=\"student_unlab_model\", \n",
        "    loader=loaders['gmm'],\n",
        ")\n",
        "student_gmm_model = CNN()\n",
        "student_gmm_model.load_state_dict(torch.load(\"student_unlab_model.pth\"))\n",
        "student_gmm_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjiFAlRaZy92",
        "outputId": "7420f649-f8bb-43ca-cbb6-f0ade49ebd5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5633\n",
            "Accuracy of the model on the test images: 81.47 %\n",
            "Validation error of the model on the test images: 0.5273\n",
            "Epoch [2/50], Loss: 0.5299\n",
            "Accuracy of the model on the test images: 83.54 %\n",
            "Validation error of the model on the test images: 0.4609\n",
            "Epoch [3/50], Loss: 0.4500\n",
            "Accuracy of the model on the test images: 84.71 %\n",
            "Validation error of the model on the test images: 0.4298\n",
            "Epoch [4/50], Loss: 0.4225\n",
            "Accuracy of the model on the test images: 86.07 %\n",
            "Validation error of the model on the test images: 0.3960\n",
            "Epoch [5/50], Loss: 0.3966\n",
            "Accuracy of the model on the test images: 86.82 %\n",
            "Validation error of the model on the test images: 0.3720\n",
            "Epoch [6/50], Loss: 0.3849\n",
            "Accuracy of the model on the test images: 87.21 %\n",
            "Validation error of the model on the test images: 0.3543\n",
            "Epoch [7/50], Loss: 0.3550\n",
            "Accuracy of the model on the test images: 87.69 %\n",
            "Validation error of the model on the test images: 0.3450\n",
            "Epoch [8/50], Loss: 0.3332\n",
            "Accuracy of the model on the test images: 88.26 %\n",
            "Validation error of the model on the test images: 0.3297\n",
            "Epoch [9/50], Loss: 0.3417\n",
            "Accuracy of the model on the test images: 88.89 %\n",
            "Validation error of the model on the test images: 0.3139\n",
            "Epoch [10/50], Loss: 0.3256\n",
            "Accuracy of the model on the test images: 89.06 %\n",
            "Validation error of the model on the test images: 0.3060\n",
            "Epoch [11/50], Loss: 0.3224\n",
            "Accuracy of the model on the test images: 89.49 %\n",
            "Validation error of the model on the test images: 0.2936\n",
            "Epoch [12/50], Loss: 0.3195\n",
            "Accuracy of the model on the test images: 90.02 %\n",
            "Validation error of the model on the test images: 0.2768\n",
            "Epoch [13/50], Loss: 0.3123\n",
            "Accuracy of the model on the test images: 90.04 %\n",
            "Validation error of the model on the test images: 0.2758\n",
            "Epoch [14/50], Loss: 0.3137\n",
            "Accuracy of the model on the test images: 90.41 %\n",
            "Validation error of the model on the test images: 0.2663\n",
            "Epoch [15/50], Loss: 0.2749\n",
            "Accuracy of the model on the test images: 90.48 %\n",
            "Validation error of the model on the test images: 0.2621\n",
            "Epoch [16/50], Loss: 0.3084\n",
            "Accuracy of the model on the test images: 91.08 %\n",
            "Validation error of the model on the test images: 0.2481\n",
            "Epoch [17/50], Loss: 0.2821\n",
            "Accuracy of the model on the test images: 91.34 %\n",
            "Validation error of the model on the test images: 0.2396\n",
            "Epoch [18/50], Loss: 0.2776\n",
            "Epoch [19/50], Loss: 0.2684\n",
            "Accuracy of the model on the test images: 91.99 %\n",
            "Validation error of the model on the test images: 0.2248\n",
            "Epoch [20/50], Loss: 0.2760\n",
            "Epoch [21/50], Loss: 0.2826\n",
            "Accuracy of the model on the test images: 91.89 %\n",
            "Validation error of the model on the test images: 0.2240\n",
            "Epoch [22/50], Loss: 0.3057\n",
            "Accuracy of the model on the test images: 92.05 %\n",
            "Validation error of the model on the test images: 0.2197\n",
            "Epoch [23/50], Loss: 0.2964\n",
            "Accuracy of the model on the test images: 92.44 %\n",
            "Validation error of the model on the test images: 0.2095\n",
            "Epoch [24/50], Loss: 0.2790\n",
            "Epoch [25/50], Loss: 0.2732\n",
            "Accuracy of the model on the test images: 92.59 %\n",
            "Validation error of the model on the test images: 0.2040\n",
            "Epoch [26/50], Loss: 0.2527\n",
            "Accuracy of the model on the test images: 93.05 %\n",
            "Validation error of the model on the test images: 0.1959\n",
            "Epoch [27/50], Loss: 0.2449\n",
            "Epoch [28/50], Loss: 0.2532\n",
            "Accuracy of the model on the test images: 93.04 %\n",
            "Validation error of the model on the test images: 0.1932\n",
            "Epoch [29/50], Loss: 0.2841\n",
            "Accuracy of the model on the test images: 93.10 %\n",
            "Validation error of the model on the test images: 0.1931\n",
            "Epoch [30/50], Loss: 0.2545\n",
            "Epoch [31/50], Loss: 0.2607\n",
            "Accuracy of the model on the test images: 93.51 %\n",
            "Validation error of the model on the test images: 0.1863\n",
            "Epoch [32/50], Loss: 0.2458\n",
            "Accuracy of the model on the test images: 93.33 %\n",
            "Validation error of the model on the test images: 0.1853\n",
            "Epoch [33/50], Loss: 0.2405\n",
            "Accuracy of the model on the test images: 93.44 %\n",
            "Validation error of the model on the test images: 0.1834\n",
            "Epoch [34/50], Loss: 0.2695\n",
            "Accuracy of the model on the test images: 93.58 %\n",
            "Validation error of the model on the test images: 0.1824\n",
            "Epoch [35/50], Loss: 0.2492\n",
            "Accuracy of the model on the test images: 93.64 %\n",
            "Validation error of the model on the test images: 0.1791\n",
            "Epoch [36/50], Loss: 0.2548\n",
            "Accuracy of the model on the test images: 93.87 %\n",
            "Validation error of the model on the test images: 0.1758\n",
            "Epoch [37/50], Loss: 0.2532\n",
            "Accuracy of the model on the test images: 93.90 %\n",
            "Validation error of the model on the test images: 0.1747\n",
            "Epoch [38/50], Loss: 0.2807\n",
            "Accuracy of the model on the test images: 93.91 %\n",
            "Validation error of the model on the test images: 0.1731\n",
            "Epoch [39/50], Loss: 0.2335\n",
            "Accuracy of the model on the test images: 94.06 %\n",
            "Validation error of the model on the test images: 0.1681\n",
            "Epoch [40/50], Loss: 0.2339\n",
            "Accuracy of the model on the test images: 93.99 %\n",
            "Validation error of the model on the test images: 0.1678\n",
            "Epoch [41/50], Loss: 0.2409\n",
            "Epoch [42/50], Loss: 0.2549\n",
            "Accuracy of the model on the test images: 94.37 %\n",
            "Validation error of the model on the test images: 0.1615\n",
            "Epoch [43/50], Loss: 0.2584\n",
            "Epoch [44/50], Loss: 0.2349\n",
            "Accuracy of the model on the test images: 94.29 %\n",
            "Validation error of the model on the test images: 0.1604\n",
            "Epoch [45/50], Loss: 0.2504\n",
            "Epoch [46/50], Loss: 0.2627\n",
            "Epoch [47/50], Loss: 0.2540\n",
            "Accuracy of the model on the test images: 94.41 %\n",
            "Validation error of the model on the test images: 0.1592\n",
            "Epoch [48/50], Loss: 0.2618\n",
            "Accuracy of the model on the test images: 94.78 %\n",
            "Validation error of the model on the test images: 0.1533\n",
            "Epoch [49/50], Loss: 0.2615\n",
            "Accuracy of the model on the test images: 94.80 %\n",
            "Validation error of the model on the test images: 0.1503\n",
            "Epoch [50/50], Loss: 0.2684\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anchor Method"
      ],
      "metadata": {
        "id": "rUO5uAtrt5vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lamda = 0.0\n",
        "distill_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * (lamda) + F.one_hot(sets['mixed'].tensors[1], num_classes=10) * (1-lamda)\n",
        "\n",
        "sets[\"distill\"] = torch.utils.data.TensorDataset(\n",
        "    sets['mixed'].tensors[0],\n",
        "    distill_labels,\n",
        ")\n",
        "\n",
        "loaders['distill'] = torch.utils.data.DataLoader(sets['distill'], batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "9Jvd2ncWt1pz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader=loaders['distill']"
      ],
      "metadata": {
        "id": "mW8xmpFK2Do_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dist(\n",
        "    name=\"student_anchor_model\", \n",
        "    loader=loaders['distill'],\n",
        ")\n",
        "student_anchor_model = CNN()\n",
        "student_anchor_model.load_state_dict(torch.load(\"student_anchor_model.pth\"))\n",
        "student_anchor_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wnemhWatx9eS",
        "outputId": "45a33035-d162-482d-a232-0a76726a188b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.0883\n",
            "Accuracy of the model on the test images: 48.77 %\n",
            "Validation error of the model on the test images: 3.2416\n",
            "Epoch [2/50], Loss: 0.1086\n",
            "Accuracy of the model on the test images: 66.27 %\n",
            "Validation error of the model on the test images: 1.4393\n",
            "Epoch [3/50], Loss: 0.1296\n",
            "Accuracy of the model on the test images: 71.32 %\n",
            "Validation error of the model on the test images: 1.0030\n",
            "Epoch [4/50], Loss: 0.0601\n",
            "Accuracy of the model on the test images: 74.85 %\n",
            "Validation error of the model on the test images: 0.8972\n",
            "Epoch [5/50], Loss: 0.0711\n",
            "Accuracy of the model on the test images: 76.16 %\n",
            "Validation error of the model on the test images: 0.8404\n",
            "Epoch [6/50], Loss: 0.0757\n",
            "Accuracy of the model on the test images: 78.10 %\n",
            "Validation error of the model on the test images: 0.7385\n",
            "Epoch [7/50], Loss: 0.0910\n",
            "Accuracy of the model on the test images: 82.91 %\n",
            "Validation error of the model on the test images: 0.5283\n",
            "Epoch [8/50], Loss: 0.0604\n",
            "Accuracy of the model on the test images: 82.71 %\n",
            "Validation error of the model on the test images: 0.5205\n",
            "Epoch [9/50], Loss: 0.0800\n",
            "Accuracy of the model on the test images: 83.76 %\n",
            "Validation error of the model on the test images: 0.4837\n",
            "Epoch [10/50], Loss: 0.0713\n",
            "Accuracy of the model on the test images: 84.82 %\n",
            "Validation error of the model on the test images: 0.4540\n",
            "Epoch [11/50], Loss: 0.0980\n",
            "Accuracy of the model on the test images: 84.82 %\n",
            "Validation error of the model on the test images: 0.4391\n",
            "Epoch [12/50], Loss: 0.0622\n",
            "Accuracy of the model on the test images: 86.44 %\n",
            "Validation error of the model on the test images: 0.3828\n",
            "Epoch [13/50], Loss: 0.0584\n",
            "Epoch [14/50], Loss: 0.0387\n",
            "Epoch [15/50], Loss: 0.0522\n",
            "Epoch [16/50], Loss: 0.0537\n",
            "Epoch [17/50], Loss: 0.0570\n",
            "Accuracy of the model on the test images: 88.55 %\n",
            "Validation error of the model on the test images: 0.3226\n",
            "Epoch [18/50], Loss: 0.0512\n",
            "Epoch [19/50], Loss: 0.0638\n",
            "Epoch [20/50], Loss: 0.0558\n",
            "Epoch [21/50], Loss: 0.0614\n",
            "Accuracy of the model on the test images: 88.87 %\n",
            "Validation error of the model on the test images: 0.3113\n",
            "Epoch [22/50], Loss: 0.0762\n",
            "Accuracy of the model on the test images: 89.58 %\n",
            "Validation error of the model on the test images: 0.2827\n",
            "Epoch [23/50], Loss: 0.0629\n",
            "Epoch [24/50], Loss: 0.0413\n",
            "Accuracy of the model on the test images: 89.90 %\n",
            "Validation error of the model on the test images: 0.2805\n",
            "Epoch [25/50], Loss: 0.0368\n",
            "Accuracy of the model on the test images: 90.38 %\n",
            "Validation error of the model on the test images: 0.2642\n",
            "Epoch [26/50], Loss: 0.0446\n",
            "Epoch [27/50], Loss: 0.0348\n",
            "Accuracy of the model on the test images: 90.56 %\n",
            "Validation error of the model on the test images: 0.2623\n",
            "Epoch [28/50], Loss: 0.0394\n",
            "Epoch [29/50], Loss: 0.0610\n",
            "Epoch [30/50], Loss: 0.0256\n",
            "Epoch [31/50], Loss: 0.0384\n",
            "Accuracy of the model on the test images: 91.03 %\n",
            "Validation error of the model on the test images: 0.2375\n",
            "Epoch [32/50], Loss: 0.0573\n",
            "Accuracy of the model on the test images: 91.69 %\n",
            "Validation error of the model on the test images: 0.2238\n",
            "Epoch [33/50], Loss: 0.0484\n",
            "Epoch [34/50], Loss: 0.0438\n",
            "Epoch [35/50], Loss: 0.0343\n",
            "Epoch [36/50], Loss: 0.0531\n",
            "Accuracy of the model on the test images: 92.23 %\n",
            "Validation error of the model on the test images: 0.2085\n",
            "Epoch [37/50], Loss: 0.0267\n",
            "Epoch [38/50], Loss: 0.0377\n",
            "Epoch [39/50], Loss: 0.0715\n",
            "Accuracy of the model on the test images: 92.39 %\n",
            "Validation error of the model on the test images: 0.1999\n",
            "Epoch [40/50], Loss: 0.0317\n",
            "Epoch [41/50], Loss: 0.0300\n",
            "Epoch [42/50], Loss: 0.0849\n",
            "Epoch [43/50], Loss: 0.0577\n",
            "Epoch [44/50], Loss: 0.0485\n",
            "Accuracy of the model on the test images: 93.19 %\n",
            "Validation error of the model on the test images: 0.1807\n",
            "Epoch [45/50], Loss: 0.0220\n",
            "Epoch [46/50], Loss: 0.0414\n",
            "Accuracy of the model on the test images: 93.27 %\n",
            "Validation error of the model on the test images: 0.1764\n",
            "Epoch [47/50], Loss: 0.0495\n",
            "Epoch [48/50], Loss: 0.0226\n",
            "Epoch [49/50], Loss: 0.0297\n",
            "Epoch [50/50], Loss: 0.0507\n",
            "Accuracy of the model on the test images: 93.66 %\n",
            "Validation error of the model on the test images: 0.1657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Baseline Method"
      ],
      "metadata": {
        "id": "fGT8US7MwA2y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train(\n",
        "    name=\"baseline_model\", \n",
        "    loader=loaders['mixed'],\n",
        ")\n",
        "baseline_model = CNN()\n",
        "baseline_model.load_state_dict(torch.load(\"baseline_model.pth\"))\n",
        "baseline_model.eval()\n",
        "None"
      ],
      "metadata": {
        "id": "jSi4nLqfw4Un",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b6986b6-9935-4bed-9114-cf37d26ea119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.0941\n",
            "Accuracy of the model on the test images: 50.11 %\n",
            "Validation error of the model on the test images: 3.2108\n",
            "Epoch [2/50], Loss: 0.1612\n",
            "Accuracy of the model on the test images: 52.31 %\n",
            "Validation error of the model on the test images: 3.0461\n",
            "Epoch [3/50], Loss: 0.1084\n",
            "Accuracy of the model on the test images: 73.00 %\n",
            "Validation error of the model on the test images: 1.0254\n",
            "Epoch [4/50], Loss: 0.0623\n",
            "Accuracy of the model on the test images: 73.57 %\n",
            "Validation error of the model on the test images: 0.9694\n",
            "Epoch [5/50], Loss: 0.0696\n",
            "Accuracy of the model on the test images: 75.88 %\n",
            "Validation error of the model on the test images: 0.8134\n",
            "Epoch [6/50], Loss: 0.0816\n",
            "Accuracy of the model on the test images: 81.35 %\n",
            "Validation error of the model on the test images: 0.5901\n",
            "Epoch [7/50], Loss: 0.0941\n",
            "Epoch [8/50], Loss: 0.0600\n",
            "Accuracy of the model on the test images: 82.92 %\n",
            "Validation error of the model on the test images: 0.5148\n",
            "Epoch [9/50], Loss: 0.0592\n",
            "Epoch [10/50], Loss: 0.0739\n",
            "Accuracy of the model on the test images: 85.04 %\n",
            "Validation error of the model on the test images: 0.4407\n",
            "Epoch [11/50], Loss: 0.0863\n",
            "Epoch [12/50], Loss: 0.0509\n",
            "Accuracy of the model on the test images: 85.91 %\n",
            "Validation error of the model on the test images: 0.4049\n",
            "Epoch [13/50], Loss: 0.0382\n",
            "Epoch [14/50], Loss: 0.0412\n",
            "Accuracy of the model on the test images: 86.98 %\n",
            "Validation error of the model on the test images: 0.3658\n",
            "Epoch [15/50], Loss: 0.0590\n",
            "Epoch [16/50], Loss: 0.0637\n",
            "Accuracy of the model on the test images: 87.47 %\n",
            "Validation error of the model on the test images: 0.3596\n",
            "Epoch [17/50], Loss: 0.0489\n",
            "Accuracy of the model on the test images: 88.16 %\n",
            "Validation error of the model on the test images: 0.3316\n",
            "Epoch [18/50], Loss: 0.0567\n",
            "Accuracy of the model on the test images: 88.43 %\n",
            "Validation error of the model on the test images: 0.3282\n",
            "Epoch [19/50], Loss: 0.0404\n",
            "Accuracy of the model on the test images: 88.91 %\n",
            "Validation error of the model on the test images: 0.3102\n",
            "Epoch [20/50], Loss: 0.0632\n",
            "Epoch [21/50], Loss: 0.0507\n",
            "Accuracy of the model on the test images: 88.92 %\n",
            "Validation error of the model on the test images: 0.3078\n",
            "Epoch [22/50], Loss: 0.0800\n",
            "Accuracy of the model on the test images: 89.89 %\n",
            "Validation error of the model on the test images: 0.2838\n",
            "Epoch [23/50], Loss: 0.0679\n",
            "Epoch [24/50], Loss: 0.0377\n",
            "Epoch [25/50], Loss: 0.0413\n",
            "Accuracy of the model on the test images: 90.23 %\n",
            "Validation error of the model on the test images: 0.2647\n",
            "Epoch [26/50], Loss: 0.0418\n",
            "Epoch [27/50], Loss: 0.0481\n",
            "Accuracy of the model on the test images: 90.79 %\n",
            "Validation error of the model on the test images: 0.2505\n",
            "Epoch [28/50], Loss: 0.0531\n",
            "Epoch [29/50], Loss: 0.0600\n",
            "Epoch [30/50], Loss: 0.0279\n",
            "Epoch [31/50], Loss: 0.0356\n",
            "Epoch [32/50], Loss: 0.0721\n",
            "Accuracy of the model on the test images: 91.44 %\n",
            "Validation error of the model on the test images: 0.2276\n",
            "Epoch [33/50], Loss: 0.0435\n",
            "Accuracy of the model on the test images: 91.90 %\n",
            "Validation error of the model on the test images: 0.2179\n",
            "Epoch [34/50], Loss: 0.0398\n",
            "Epoch [35/50], Loss: 0.0368\n",
            "Accuracy of the model on the test images: 92.40 %\n",
            "Validation error of the model on the test images: 0.2028\n",
            "Epoch [36/50], Loss: 0.0688\n",
            "Epoch [37/50], Loss: 0.0520\n",
            "Accuracy of the model on the test images: 92.63 %\n",
            "Validation error of the model on the test images: 0.1980\n",
            "Epoch [38/50], Loss: 0.0195\n",
            "Epoch [39/50], Loss: 0.0556\n",
            "Epoch [40/50], Loss: 0.0230\n",
            "Accuracy of the model on the test images: 92.46 %\n",
            "Validation error of the model on the test images: 0.1970\n",
            "Epoch [41/50], Loss: 0.0342\n",
            "Epoch [42/50], Loss: 0.0738\n",
            "Accuracy of the model on the test images: 92.48 %\n",
            "Validation error of the model on the test images: 0.1956\n",
            "Epoch [43/50], Loss: 0.0455\n",
            "Accuracy of the model on the test images: 92.78 %\n",
            "Validation error of the model on the test images: 0.1920\n",
            "Epoch [44/50], Loss: 0.0424\n",
            "Accuracy of the model on the test images: 93.03 %\n",
            "Validation error of the model on the test images: 0.1836\n",
            "Epoch [45/50], Loss: 0.0335\n",
            "Accuracy of the model on the test images: 93.24 %\n",
            "Validation error of the model on the test images: 0.1803\n",
            "Epoch [46/50], Loss: 0.0393\n",
            "Epoch [47/50], Loss: 0.0399\n",
            "Epoch [48/50], Loss: 0.0291\n",
            "Accuracy of the model on the test images: 93.29 %\n",
            "Validation error of the model on the test images: 0.1770\n",
            "Epoch [49/50], Loss: 0.0348\n",
            "Accuracy of the model on the test images: 93.57 %\n",
            "Validation error of the model on the test images: 0.1682\n",
            "Epoch [50/50], Loss: 0.0439\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred, true):\n",
        "    return 100 * (pred==true).sum() / pred.shape[0]\n",
        "\n",
        "def get_metrics(targets, loader):\n",
        "    with torch.no_grad():\n",
        "        good_churn = GoodBadChurn(mode=\"good\", tensor_type=\"torch\", output_mode = \"count\")\n",
        "        bad_churn = GoodBadChurn(mode=\"bad\", tensor_type=\"torch\", output_mode = \"count\")\n",
        "        pred_teacher = []\n",
        "        pred_baseline = []\n",
        "        pred_distill = []\n",
        "        pred_gmm = []\n",
        "        for (x,y) in tqdm(loader):\n",
        "            x = x.unsqueeze(1).float()\n",
        "            pred = teacher_model(x)\n",
        "            pred_teacher.append(pred)\n",
        "            pred = baseline_model(x)\n",
        "            pred_baseline.append(pred)\n",
        "            pred = student_anchor_model(x)\n",
        "            pred_distill.append(pred)\n",
        "            pred = student_gmm_model(x)\n",
        "            pred_gmm.append(pred)\n",
        "        pred_teacher = torch.concatenate(pred_teacher).argmax(dim=1)\n",
        "        pred_baseline = torch.concatenate(pred_baseline).argmax(dim=1)\n",
        "        pred_distill = torch.concatenate(pred_distill).argmax(dim=1)\n",
        "        pred_gmm = torch.concatenate(pred_gmm).argmax(dim=1)\n",
        "        \n",
        "        gchurns = {\n",
        "            \"baseline\": good_churn(targets, pred_teacher, pred_baseline).item(),\n",
        "            \"add_unlab\": good_churn(targets, pred_teacher, pred_gmm).item(),\n",
        "            \"anchor\": good_churn(targets, pred_teacher, pred_distill).item(),\n",
        "        }\n",
        "        bchurns = {\n",
        "            \"baseline\": bad_churn(targets, pred_teacher, pred_baseline).item(),\n",
        "            \"add_unlab\": bad_churn(targets, pred_teacher, pred_gmm).item(),\n",
        "            \"anchor\": bad_churn(targets, pred_teacher, pred_distill).item(),\n",
        "        }\n",
        "        accuracies = {\n",
        "            \"baseline\": accuracy(pred_baseline, targets).item(),\n",
        "            \"add_unlab\": accuracy(pred_gmm, targets).item(),\n",
        "            \"anchor\": accuracy(pred_distill, targets).item(),\n",
        "            \"teacher\": accuracy(pred_teacher, targets).item(),\n",
        "        }\n",
        "        return gchurns, bchurns, accuracies\n",
        "        "
      ],
      "metadata": {
        "id": "cYGf4fjUyFTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_dict = {   \"gchurns_val\":  {'baseline': 0, 'add_unlab': 0, 'anchor': 0},\n",
        "                  \"bchurns_val\":  {'baseline': 0, 'add_unlab': 0, 'anchor': 0}, \n",
        "                  \"accuracies_val\": {'baseline': 0, 'add_unlab': 0, 'anchor': 0}}\n",
        "\n",
        "metrics_dict_multiple = {\n",
        "    x: metrics_dict.copy()\n",
        "    for x in [0.0, 0.3, 0.5, 0.7, 0.9]\n",
        "}"
      ],
      "metadata": {
        "id": "lcn6iRPFKHX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gchurns_mixed, bchurns_mixed, accuracies_mixed = get_metrics(sets['mixed'].tensors[1], loaders['mixed'])\n",
        "# metrics_dict_multiple[0.0][\"gchurns_mixed\"] = gchurns_mixed\n",
        "# metrics_dict_multiple[0.0][\"bchurns_mixed\"] = bchurns_mixed\n",
        "# metrics_dict_multiple[0.0][\"accuracies_mixed\"] = accuracies_mixed"
      ],
      "metadata": {
        "id": "p4sBSKm_0eOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gchurns_val, bchurns_val, accuracies_val = get_metrics(sets['val'].tensors[1], loaders['val'])\n",
        "metrics_dict_multiple[0.0][\"gchurns_val\"] = gchurns_val\n",
        "metrics_dict_multiple[0.0][\"bchurns_val\"] = bchurns_val\n",
        "metrics_dict_multiple[0.0][\"accuracies_val\"] = accuracies_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLqXOAZnVn7-",
        "outputId": "3e2cdfba-6a83-4608-ebbd-d9eacf6e6909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:19<00:00,  1.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lambda = 0.3"
      ],
      "metadata": {
        "id": "7f6uCTH8Lmqm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relabel based on threshold\n",
        "confidence_threshold = 1.0\n",
        "\n",
        "# gmm_labels = teacher_labels_mixed\n",
        "lamda = 0.3\n",
        "\n",
        "mixed_label = F.one_hot(sets['mixed'].tensors[1], num_classes=10)\n",
        "gmm_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * lamda + mixed_label * (1 - lamda)\n",
        "gmm_labels = torch.where(mixed_label != F.one_hot(y_mixed, num_classes=10), mixed_label, gmm_labels)\n",
        "\n",
        "\n",
        "sets[\"teacher_unlab\"] = torch.utils.data.TensorDataset(\n",
        "    total_dict['x'][np.isin(total_dict['field'], [\"unlab\"])],\n",
        "    unlab_teacherpred\n",
        ")\n",
        "\n",
        "sets[\"gmm\"] = torch.utils.data.TensorDataset(\n",
        "    torch.concat([\n",
        "        sets['mixed'].tensors[0],\n",
        "        sets['unlab'].tensors[0],\n",
        "    ]),\n",
        "    torch.concat([\n",
        "        gmm_labels,\n",
        "       unlab_teacherpred\n",
        "    ]),\n",
        ")\n",
        "loaders['gmm'] = torch.utils.data.DataLoader(sets[\"gmm\"], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_dist(\n",
        "    name=\"student_unlab_model\", \n",
        "    loader=loaders['gmm'],\n",
        ")\n",
        "student_gmm_model = CNN()\n",
        "student_gmm_model.load_state_dict(torch.load(\"student_unlab_model.pth\"))\n",
        "student_gmm_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08vDFkTfQchM",
        "outputId": "f5e53856-5da8-4b0e-a2ba-1368b87dc797"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5497\n",
            "Accuracy of the model on the test images: 82.22 %\n",
            "Validation error of the model on the test images: 0.5053\n",
            "Epoch [2/50], Loss: 0.4863\n",
            "Accuracy of the model on the test images: 84.52 %\n",
            "Validation error of the model on the test images: 0.4288\n",
            "Epoch [3/50], Loss: 0.4366\n",
            "Accuracy of the model on the test images: 86.25 %\n",
            "Validation error of the model on the test images: 0.3903\n",
            "Epoch [4/50], Loss: 0.4056\n",
            "Accuracy of the model on the test images: 86.45 %\n",
            "Validation error of the model on the test images: 0.3766\n",
            "Epoch [5/50], Loss: 0.3507\n",
            "Accuracy of the model on the test images: 87.54 %\n",
            "Validation error of the model on the test images: 0.3415\n",
            "Epoch [6/50], Loss: 0.3635\n",
            "Accuracy of the model on the test images: 87.87 %\n",
            "Validation error of the model on the test images: 0.3286\n",
            "Epoch [7/50], Loss: 0.3474\n",
            "Accuracy of the model on the test images: 88.62 %\n",
            "Validation error of the model on the test images: 0.3096\n",
            "Epoch [8/50], Loss: 0.3261\n",
            "Accuracy of the model on the test images: 89.24 %\n",
            "Validation error of the model on the test images: 0.2930\n",
            "Epoch [9/50], Loss: 0.3142\n",
            "Accuracy of the model on the test images: 89.64 %\n",
            "Validation error of the model on the test images: 0.2856\n",
            "Epoch [10/50], Loss: 0.2869\n",
            "Accuracy of the model on the test images: 89.79 %\n",
            "Validation error of the model on the test images: 0.2781\n",
            "Epoch [11/50], Loss: 0.2816\n",
            "Accuracy of the model on the test images: 90.09 %\n",
            "Validation error of the model on the test images: 0.2678\n",
            "Epoch [12/50], Loss: 0.3018\n",
            "Accuracy of the model on the test images: 90.64 %\n",
            "Validation error of the model on the test images: 0.2557\n",
            "Epoch [13/50], Loss: 0.2777\n",
            "Accuracy of the model on the test images: 90.77 %\n",
            "Validation error of the model on the test images: 0.2471\n",
            "Epoch [14/50], Loss: 0.2947\n",
            "Accuracy of the model on the test images: 91.06 %\n",
            "Validation error of the model on the test images: 0.2419\n",
            "Epoch [15/50], Loss: 0.2766\n",
            "Accuracy of the model on the test images: 91.14 %\n",
            "Validation error of the model on the test images: 0.2384\n",
            "Epoch [16/50], Loss: 0.3033\n",
            "Accuracy of the model on the test images: 91.20 %\n",
            "Validation error of the model on the test images: 0.2342\n",
            "Epoch [17/50], Loss: 0.2777\n",
            "Accuracy of the model on the test images: 91.70 %\n",
            "Validation error of the model on the test images: 0.2253\n",
            "Epoch [18/50], Loss: 0.2614\n",
            "Accuracy of the model on the test images: 91.73 %\n",
            "Validation error of the model on the test images: 0.2225\n",
            "Epoch [19/50], Loss: 0.2669\n",
            "Accuracy of the model on the test images: 92.20 %\n",
            "Validation error of the model on the test images: 0.2138\n",
            "Epoch [20/50], Loss: 0.2707\n",
            "Accuracy of the model on the test images: 92.22 %\n",
            "Validation error of the model on the test images: 0.2133\n",
            "Epoch [21/50], Loss: 0.2795\n",
            "Accuracy of the model on the test images: 92.05 %\n",
            "Validation error of the model on the test images: 0.2123\n",
            "Epoch [22/50], Loss: 0.2766\n",
            "Accuracy of the model on the test images: 92.34 %\n",
            "Validation error of the model on the test images: 0.2072\n",
            "Epoch [23/50], Loss: 0.2724\n",
            "Accuracy of the model on the test images: 92.56 %\n",
            "Validation error of the model on the test images: 0.1998\n",
            "Epoch [24/50], Loss: 0.2563\n",
            "Accuracy of the model on the test images: 92.86 %\n",
            "Validation error of the model on the test images: 0.1962\n",
            "Epoch [25/50], Loss: 0.2905\n",
            "Epoch [26/50], Loss: 0.2511\n",
            "Accuracy of the model on the test images: 92.89 %\n",
            "Validation error of the model on the test images: 0.1939\n",
            "Epoch [27/50], Loss: 0.2559\n",
            "Accuracy of the model on the test images: 92.98 %\n",
            "Validation error of the model on the test images: 0.1920\n",
            "Epoch [28/50], Loss: 0.2503\n",
            "Accuracy of the model on the test images: 93.11 %\n",
            "Validation error of the model on the test images: 0.1903\n",
            "Epoch [29/50], Loss: 0.2541\n",
            "Accuracy of the model on the test images: 93.20 %\n",
            "Validation error of the model on the test images: 0.1855\n",
            "Epoch [30/50], Loss: 0.2628\n",
            "Epoch [31/50], Loss: 0.2607\n",
            "Accuracy of the model on the test images: 93.37 %\n",
            "Validation error of the model on the test images: 0.1805\n",
            "Epoch [32/50], Loss: 0.2363\n",
            "Accuracy of the model on the test images: 93.32 %\n",
            "Validation error of the model on the test images: 0.1785\n",
            "Epoch [33/50], Loss: 0.2507\n",
            "Epoch [34/50], Loss: 0.2600\n",
            "Accuracy of the model on the test images: 93.66 %\n",
            "Validation error of the model on the test images: 0.1725\n",
            "Epoch [35/50], Loss: 0.2444\n",
            "Epoch [36/50], Loss: 0.2453\n",
            "Epoch [37/50], Loss: 0.2623\n",
            "Accuracy of the model on the test images: 93.90 %\n",
            "Validation error of the model on the test images: 0.1699\n",
            "Epoch [38/50], Loss: 0.2643\n",
            "Accuracy of the model on the test images: 93.98 %\n",
            "Validation error of the model on the test images: 0.1676\n",
            "Epoch [39/50], Loss: 0.2213\n",
            "Accuracy of the model on the test images: 94.06 %\n",
            "Validation error of the model on the test images: 0.1641\n",
            "Epoch [40/50], Loss: 0.2405\n",
            "Accuracy of the model on the test images: 94.20 %\n",
            "Validation error of the model on the test images: 0.1604\n",
            "Epoch [41/50], Loss: 0.2342\n",
            "Epoch [42/50], Loss: 0.2532\n",
            "Epoch [43/50], Loss: 0.2457\n",
            "Accuracy of the model on the test images: 94.32 %\n",
            "Validation error of the model on the test images: 0.1576\n",
            "Epoch [44/50], Loss: 0.2393\n",
            "Epoch [45/50], Loss: 0.2520\n",
            "Accuracy of the model on the test images: 94.46 %\n",
            "Validation error of the model on the test images: 0.1560\n",
            "Epoch [46/50], Loss: 0.2493\n",
            "Accuracy of the model on the test images: 94.52 %\n",
            "Validation error of the model on the test images: 0.1526\n",
            "Epoch [47/50], Loss: 0.2455\n",
            "Epoch [48/50], Loss: 0.2537\n",
            "Accuracy of the model on the test images: 94.64 %\n",
            "Validation error of the model on the test images: 0.1507\n",
            "Epoch [49/50], Loss: 0.2367\n",
            "Epoch [50/50], Loss: 0.2599\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lamda = 0.3\n",
        "distill_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * (lamda) + F.one_hot(sets['mixed'].tensors[1], num_classes=10) * (1-lamda)\n",
        "\n",
        "sets[\"distill\"] = torch.utils.data.TensorDataset(\n",
        "    sets['mixed'].tensors[0],\n",
        "    distill_labels,\n",
        ")\n",
        "\n",
        "loaders['distill'] = torch.utils.data.DataLoader(sets['distill'], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loader=loaders['distill']\n",
        "\n",
        "train_dist(\n",
        "    name=\"student_anchor_model\", \n",
        "    loader=loaders['distill'],\n",
        ")\n",
        "student_anchor_model = CNN()\n",
        "student_anchor_model.load_state_dict(torch.load(\"student_anchor_model.pth\"))\n",
        "student_anchor_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoIEcP_omM5J",
        "outputId": "faf54ca1-fa5c-48d2-cef0-eee57e2fa9bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.1264\n",
            "Accuracy of the model on the test images: 61.25 %\n",
            "Validation error of the model on the test images: 1.6948\n",
            "Epoch [2/50], Loss: 0.1482\n",
            "Accuracy of the model on the test images: 64.87 %\n",
            "Validation error of the model on the test images: 1.4355\n",
            "Epoch [3/50], Loss: 0.1389\n",
            "Accuracy of the model on the test images: 76.58 %\n",
            "Validation error of the model on the test images: 0.7517\n",
            "Epoch [4/50], Loss: 0.0900\n",
            "Epoch [5/50], Loss: 0.0975\n",
            "Accuracy of the model on the test images: 80.54 %\n",
            "Validation error of the model on the test images: 0.6082\n",
            "Epoch [6/50], Loss: 0.0933\n",
            "Accuracy of the model on the test images: 81.91 %\n",
            "Validation error of the model on the test images: 0.5737\n",
            "Epoch [7/50], Loss: 0.1394\n",
            "Epoch [8/50], Loss: 0.0777\n",
            "Accuracy of the model on the test images: 84.28 %\n",
            "Validation error of the model on the test images: 0.4541\n",
            "Epoch [9/50], Loss: 0.0966\n",
            "Epoch [10/50], Loss: 0.0958\n",
            "Accuracy of the model on the test images: 85.38 %\n",
            "Validation error of the model on the test images: 0.4186\n",
            "Epoch [11/50], Loss: 0.0902\n",
            "Accuracy of the model on the test images: 86.63 %\n",
            "Validation error of the model on the test images: 0.3731\n",
            "Epoch [12/50], Loss: 0.0813\n",
            "Accuracy of the model on the test images: 87.30 %\n",
            "Validation error of the model on the test images: 0.3597\n",
            "Epoch [13/50], Loss: 0.0727\n",
            "Epoch [14/50], Loss: 0.0791\n",
            "Accuracy of the model on the test images: 88.51 %\n",
            "Validation error of the model on the test images: 0.3171\n",
            "Epoch [15/50], Loss: 0.0633\n",
            "Epoch [16/50], Loss: 0.0768\n",
            "Accuracy of the model on the test images: 89.21 %\n",
            "Validation error of the model on the test images: 0.2927\n",
            "Epoch [17/50], Loss: 0.0760\n",
            "Epoch [18/50], Loss: 0.0593\n",
            "Epoch [19/50], Loss: 0.0583\n",
            "Accuracy of the model on the test images: 89.73 %\n",
            "Validation error of the model on the test images: 0.2788\n",
            "Epoch [20/50], Loss: 0.0731\n",
            "Epoch [21/50], Loss: 0.0559\n",
            "Epoch [22/50], Loss: 0.1071\n",
            "Accuracy of the model on the test images: 90.56 %\n",
            "Validation error of the model on the test images: 0.2493\n",
            "Epoch [23/50], Loss: 0.0714\n",
            "Accuracy of the model on the test images: 90.75 %\n",
            "Validation error of the model on the test images: 0.2487\n",
            "Epoch [24/50], Loss: 0.0616\n",
            "Accuracy of the model on the test images: 91.05 %\n",
            "Validation error of the model on the test images: 0.2380\n",
            "Epoch [25/50], Loss: 0.0507\n",
            "Epoch [26/50], Loss: 0.0635\n",
            "Epoch [27/50], Loss: 0.0757\n",
            "Accuracy of the model on the test images: 91.66 %\n",
            "Validation error of the model on the test images: 0.2206\n",
            "Epoch [28/50], Loss: 0.0545\n",
            "Accuracy of the model on the test images: 91.84 %\n",
            "Validation error of the model on the test images: 0.2125\n",
            "Epoch [29/50], Loss: 0.0671\n",
            "Epoch [30/50], Loss: 0.0696\n",
            "Epoch [31/50], Loss: 0.0552\n",
            "Accuracy of the model on the test images: 92.14 %\n",
            "Validation error of the model on the test images: 0.2042\n",
            "Epoch [32/50], Loss: 0.0873\n",
            "Epoch [33/50], Loss: 0.0747\n",
            "Accuracy of the model on the test images: 92.55 %\n",
            "Validation error of the model on the test images: 0.1936\n",
            "Epoch [34/50], Loss: 0.0682\n",
            "Epoch [35/50], Loss: 0.0552\n",
            "Epoch [36/50], Loss: 0.0802\n",
            "Accuracy of the model on the test images: 93.07 %\n",
            "Validation error of the model on the test images: 0.1808\n",
            "Epoch [37/50], Loss: 0.0592\n",
            "Epoch [38/50], Loss: 0.0566\n",
            "Epoch [39/50], Loss: 0.0632\n",
            "Accuracy of the model on the test images: 93.20 %\n",
            "Validation error of the model on the test images: 0.1790\n",
            "Epoch [40/50], Loss: 0.0565\n",
            "Accuracy of the model on the test images: 93.10 %\n",
            "Validation error of the model on the test images: 0.1778\n",
            "Epoch [41/50], Loss: 0.0490\n",
            "Epoch [42/50], Loss: 0.0912\n",
            "Epoch [43/50], Loss: 0.0547\n",
            "Epoch [44/50], Loss: 0.0804\n",
            "Accuracy of the model on the test images: 93.94 %\n",
            "Validation error of the model on the test images: 0.1573\n",
            "Epoch [45/50], Loss: 0.0738\n",
            "Epoch [46/50], Loss: 0.0470\n",
            "Epoch [47/50], Loss: 0.0777\n",
            "Epoch [48/50], Loss: 0.0504\n",
            "Epoch [49/50], Loss: 0.0716\n",
            "Accuracy of the model on the test images: 94.20 %\n",
            "Validation error of the model on the test images: 0.1499\n",
            "Epoch [50/50], Loss: 0.0661\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gchurns_mixed, bchurns_mixed, accuracies_mixed = get_metrics(sets['mixed'].tensors[1], loaders['mixed'])\n",
        "# metrics_dict_multiple[0.3][\"gchurns_mixed\"] = gchurns_mixed\n",
        "# metrics_dict_multiple[0.3][\"bchurns_mixed\"] = bchurns_mixed\n",
        "# metrics_dict_multiple[0.3][\"accuracies_mixed\"] = accuracies_mixed\n",
        "\n",
        "gchurns_val, bchurns_val, accuracies_val = get_metrics(sets['val'].tensors[1], loaders['val'])\n",
        "metrics_dict_multiple[0.3][\"gchurns_val\"] = gchurns_val\n",
        "metrics_dict_multiple[0.3][\"bchurns_val\"] = bchurns_val\n",
        "metrics_dict_multiple[0.3][\"accuracies_val\"] = accuracies_val\n"
      ],
      "metadata": {
        "id": "J50R1pF3Lj51",
        "outputId": "b7c76187-457f-483a-b7b7-b2be928c270a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:22<00:00,  1.43it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lambda = 0.5"
      ],
      "metadata": {
        "id": "frBdmmVim-Zz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relabel based on threshold\n",
        "confidence_threshold = 1.0\n",
        "\n",
        "# gmm_labels = teacher_labels_mixed\n",
        "lamda = 0.5\n",
        "\n",
        "mixed_label = F.one_hot(sets['mixed'].tensors[1], num_classes=10)\n",
        "gmm_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * lamda + mixed_label * (1 - lamda)\n",
        "gmm_labels = torch.where(mixed_label != F.one_hot(y_mixed, num_classes=10), mixed_label, gmm_labels)\n",
        "\n",
        "\n",
        "sets[\"teacher_unlab\"] = torch.utils.data.TensorDataset(\n",
        "    total_dict['x'][np.isin(total_dict['field'], [\"unlab\"])],\n",
        "    unlab_teacherpred\n",
        ")\n",
        "\n",
        "sets[\"gmm\"] = torch.utils.data.TensorDataset(\n",
        "    torch.concat([\n",
        "        sets['mixed'].tensors[0],\n",
        "        sets['unlab'].tensors[0],\n",
        "    ]),\n",
        "    torch.concat([\n",
        "        gmm_labels,\n",
        "       unlab_teacherpred\n",
        "    ]),\n",
        ")\n",
        "loaders['gmm'] = torch.utils.data.DataLoader(sets[\"gmm\"], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "\n",
        "train_dist(\n",
        "    name=\"student_unlab_model\", \n",
        "    loader=loaders['gmm'],\n",
        ")\n",
        "student_gmm_model = CNN()\n",
        "student_gmm_model.load_state_dict(torch.load(\"student_unlab_model.pth\"))\n",
        "student_gmm_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5h3vTOj_m87I",
        "outputId": "f75e8129-3868-4f49-a33e-811726b8226b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5675\n",
            "Accuracy of the model on the test images: 82.57 %\n",
            "Validation error of the model on the test images: 0.4909\n",
            "Epoch [2/50], Loss: 0.4880\n",
            "Accuracy of the model on the test images: 85.39 %\n",
            "Validation error of the model on the test images: 0.4120\n",
            "Epoch [3/50], Loss: 0.4470\n",
            "Accuracy of the model on the test images: 86.58 %\n",
            "Validation error of the model on the test images: 0.3800\n",
            "Epoch [4/50], Loss: 0.3906\n",
            "Accuracy of the model on the test images: 87.72 %\n",
            "Validation error of the model on the test images: 0.3460\n",
            "Epoch [5/50], Loss: 0.3348\n",
            "Accuracy of the model on the test images: 88.42 %\n",
            "Validation error of the model on the test images: 0.3287\n",
            "Epoch [6/50], Loss: 0.3459\n",
            "Accuracy of the model on the test images: 88.14 %\n",
            "Validation error of the model on the test images: 0.3258\n",
            "Epoch [7/50], Loss: 0.3240\n",
            "Accuracy of the model on the test images: 89.40 %\n",
            "Validation error of the model on the test images: 0.2965\n",
            "Epoch [8/50], Loss: 0.3303\n",
            "Accuracy of the model on the test images: 89.92 %\n",
            "Validation error of the model on the test images: 0.2825\n",
            "Epoch [9/50], Loss: 0.3114\n",
            "Accuracy of the model on the test images: 90.15 %\n",
            "Validation error of the model on the test images: 0.2763\n",
            "Epoch [10/50], Loss: 0.2916\n",
            "Accuracy of the model on the test images: 90.58 %\n",
            "Validation error of the model on the test images: 0.2635\n",
            "Epoch [11/50], Loss: 0.2842\n",
            "Accuracy of the model on the test images: 90.74 %\n",
            "Validation error of the model on the test images: 0.2575\n",
            "Epoch [12/50], Loss: 0.2905\n",
            "Accuracy of the model on the test images: 91.06 %\n",
            "Validation error of the model on the test images: 0.2457\n",
            "Epoch [13/50], Loss: 0.2693\n",
            "Accuracy of the model on the test images: 91.35 %\n",
            "Validation error of the model on the test images: 0.2384\n",
            "Epoch [14/50], Loss: 0.2899\n",
            "Accuracy of the model on the test images: 91.51 %\n",
            "Validation error of the model on the test images: 0.2345\n",
            "Epoch [15/50], Loss: 0.2785\n",
            "Accuracy of the model on the test images: 91.77 %\n",
            "Validation error of the model on the test images: 0.2266\n",
            "Epoch [16/50], Loss: 0.3205\n",
            "Accuracy of the model on the test images: 91.81 %\n",
            "Validation error of the model on the test images: 0.2245\n",
            "Epoch [17/50], Loss: 0.2722\n",
            "Accuracy of the model on the test images: 92.21 %\n",
            "Validation error of the model on the test images: 0.2175\n",
            "Epoch [18/50], Loss: 0.2669\n",
            "Accuracy of the model on the test images: 92.51 %\n",
            "Validation error of the model on the test images: 0.2098\n",
            "Epoch [19/50], Loss: 0.2724\n",
            "Accuracy of the model on the test images: 92.77 %\n",
            "Validation error of the model on the test images: 0.2028\n",
            "Epoch [20/50], Loss: 0.2786\n",
            "Epoch [21/50], Loss: 0.2688\n",
            "Accuracy of the model on the test images: 92.79 %\n",
            "Validation error of the model on the test images: 0.2011\n",
            "Epoch [22/50], Loss: 0.2961\n",
            "Accuracy of the model on the test images: 92.86 %\n",
            "Validation error of the model on the test images: 0.2002\n",
            "Epoch [23/50], Loss: 0.2598\n",
            "Accuracy of the model on the test images: 93.29 %\n",
            "Validation error of the model on the test images: 0.1897\n",
            "Epoch [24/50], Loss: 0.2454\n",
            "Accuracy of the model on the test images: 93.27 %\n",
            "Validation error of the model on the test images: 0.1888\n",
            "Epoch [25/50], Loss: 0.2735\n",
            "Accuracy of the model on the test images: 93.15 %\n",
            "Validation error of the model on the test images: 0.1887\n",
            "Epoch [26/50], Loss: 0.2414\n",
            "Accuracy of the model on the test images: 93.51 %\n",
            "Validation error of the model on the test images: 0.1858\n",
            "Epoch [27/50], Loss: 0.2607\n",
            "Accuracy of the model on the test images: 93.62 %\n",
            "Validation error of the model on the test images: 0.1820\n",
            "Epoch [28/50], Loss: 0.2527\n",
            "Accuracy of the model on the test images: 93.76 %\n",
            "Validation error of the model on the test images: 0.1794\n",
            "Epoch [29/50], Loss: 0.2579\n",
            "Accuracy of the model on the test images: 93.84 %\n",
            "Validation error of the model on the test images: 0.1740\n",
            "Epoch [30/50], Loss: 0.2552\n",
            "Epoch [31/50], Loss: 0.2548\n",
            "Accuracy of the model on the test images: 93.92 %\n",
            "Validation error of the model on the test images: 0.1721\n",
            "Epoch [32/50], Loss: 0.2370\n",
            "Accuracy of the model on the test images: 94.12 %\n",
            "Validation error of the model on the test images: 0.1694\n",
            "Epoch [33/50], Loss: 0.2209\n",
            "Epoch [34/50], Loss: 0.2619\n",
            "Accuracy of the model on the test images: 94.35 %\n",
            "Validation error of the model on the test images: 0.1643\n",
            "Epoch [35/50], Loss: 0.2354\n",
            "Accuracy of the model on the test images: 94.33 %\n",
            "Validation error of the model on the test images: 0.1631\n",
            "Epoch [36/50], Loss: 0.2459\n",
            "Epoch [37/50], Loss: 0.2491\n",
            "Accuracy of the model on the test images: 94.50 %\n",
            "Validation error of the model on the test images: 0.1608\n",
            "Epoch [38/50], Loss: 0.2650\n",
            "Accuracy of the model on the test images: 94.55 %\n",
            "Validation error of the model on the test images: 0.1602\n",
            "Epoch [39/50], Loss: 0.2301\n",
            "Accuracy of the model on the test images: 94.63 %\n",
            "Validation error of the model on the test images: 0.1560\n",
            "Epoch [40/50], Loss: 0.2340\n",
            "Epoch [41/50], Loss: 0.2307\n",
            "Accuracy of the model on the test images: 94.73 %\n",
            "Validation error of the model on the test images: 0.1545\n",
            "Epoch [42/50], Loss: 0.2345\n",
            "Accuracy of the model on the test images: 94.72 %\n",
            "Validation error of the model on the test images: 0.1530\n",
            "Epoch [43/50], Loss: 0.2456\n",
            "Accuracy of the model on the test images: 94.79 %\n",
            "Validation error of the model on the test images: 0.1522\n",
            "Epoch [44/50], Loss: 0.2492\n",
            "Epoch [45/50], Loss: 0.2512\n",
            "Accuracy of the model on the test images: 94.99 %\n",
            "Validation error of the model on the test images: 0.1487\n",
            "Epoch [46/50], Loss: 0.2383\n",
            "Accuracy of the model on the test images: 95.06 %\n",
            "Validation error of the model on the test images: 0.1468\n",
            "Epoch [47/50], Loss: 0.2423\n",
            "Epoch [48/50], Loss: 0.2542\n",
            "Accuracy of the model on the test images: 95.10 %\n",
            "Validation error of the model on the test images: 0.1455\n",
            "Epoch [49/50], Loss: 0.2339\n",
            "Epoch [50/50], Loss: 0.2488\n",
            "Accuracy of the model on the test images: 95.17 %\n",
            "Validation error of the model on the test images: 0.1448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lamda = 0.5\n",
        "distill_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * (lamda) + F.one_hot(sets['mixed'].tensors[1], num_classes=10) * (1-lamda)\n",
        "\n",
        "sets[\"distill\"] = torch.utils.data.TensorDataset(\n",
        "    sets['mixed'].tensors[0],\n",
        "    distill_labels,\n",
        ")\n",
        "\n",
        "loaders['distill'] = torch.utils.data.DataLoader(sets['distill'], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loader=loaders['distill']\n",
        "\n",
        "train_dist(\n",
        "    name=\"student_anchor_model\", \n",
        "    loader=loaders['distill'],\n",
        ")\n",
        "student_anchor_model = CNN()\n",
        "student_anchor_model.load_state_dict(torch.load(\"student_anchor_model.pth\"))\n",
        "student_anchor_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeAmDdyinb-4",
        "outputId": "bd10b6e1-501c-44af-bb8f-31e8a730f49f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.1431\n",
            "Accuracy of the model on the test images: 63.44 %\n",
            "Validation error of the model on the test images: 1.4643\n",
            "Epoch [2/50], Loss: 0.1623\n",
            "Accuracy of the model on the test images: 74.72 %\n",
            "Validation error of the model on the test images: 0.8787\n",
            "Epoch [3/50], Loss: 0.1342\n",
            "Accuracy of the model on the test images: 77.62 %\n",
            "Validation error of the model on the test images: 0.6833\n",
            "Epoch [4/50], Loss: 0.0885\n",
            "Accuracy of the model on the test images: 81.47 %\n",
            "Validation error of the model on the test images: 0.5857\n",
            "Epoch [5/50], Loss: 0.1006\n",
            "Accuracy of the model on the test images: 82.76 %\n",
            "Validation error of the model on the test images: 0.5438\n",
            "Epoch [6/50], Loss: 0.0955\n",
            "Accuracy of the model on the test images: 84.91 %\n",
            "Validation error of the model on the test images: 0.4407\n",
            "Epoch [7/50], Loss: 0.1084\n",
            "Epoch [8/50], Loss: 0.0775\n",
            "Accuracy of the model on the test images: 86.00 %\n",
            "Validation error of the model on the test images: 0.4160\n",
            "Epoch [9/50], Loss: 0.0892\n",
            "Accuracy of the model on the test images: 86.66 %\n",
            "Validation error of the model on the test images: 0.4004\n",
            "Epoch [10/50], Loss: 0.1116\n",
            "Accuracy of the model on the test images: 87.22 %\n",
            "Validation error of the model on the test images: 0.3796\n",
            "Epoch [11/50], Loss: 0.0826\n",
            "Accuracy of the model on the test images: 87.13 %\n",
            "Validation error of the model on the test images: 0.3616\n",
            "Epoch [12/50], Loss: 0.0964\n",
            "Accuracy of the model on the test images: 88.93 %\n",
            "Validation error of the model on the test images: 0.3168\n",
            "Epoch [13/50], Loss: 0.0838\n",
            "Epoch [14/50], Loss: 0.0859\n",
            "Accuracy of the model on the test images: 89.72 %\n",
            "Validation error of the model on the test images: 0.2896\n",
            "Epoch [15/50], Loss: 0.0632\n",
            "Epoch [16/50], Loss: 0.0921\n",
            "Accuracy of the model on the test images: 89.80 %\n",
            "Validation error of the model on the test images: 0.2848\n",
            "Epoch [17/50], Loss: 0.0805\n",
            "Accuracy of the model on the test images: 90.52 %\n",
            "Validation error of the model on the test images: 0.2600\n",
            "Epoch [18/50], Loss: 0.0664\n",
            "Epoch [19/50], Loss: 0.0676\n",
            "Accuracy of the model on the test images: 90.62 %\n",
            "Validation error of the model on the test images: 0.2596\n",
            "Epoch [20/50], Loss: 0.0869\n",
            "Accuracy of the model on the test images: 90.83 %\n",
            "Validation error of the model on the test images: 0.2503\n",
            "Epoch [21/50], Loss: 0.0656\n",
            "Accuracy of the model on the test images: 91.04 %\n",
            "Validation error of the model on the test images: 0.2421\n",
            "Epoch [22/50], Loss: 0.0958\n",
            "Accuracy of the model on the test images: 91.56 %\n",
            "Validation error of the model on the test images: 0.2299\n",
            "Epoch [23/50], Loss: 0.0873\n",
            "Accuracy of the model on the test images: 91.69 %\n",
            "Validation error of the model on the test images: 0.2290\n",
            "Epoch [24/50], Loss: 0.0742\n",
            "Accuracy of the model on the test images: 92.05 %\n",
            "Validation error of the model on the test images: 0.2182\n",
            "Epoch [25/50], Loss: 0.0583\n",
            "Epoch [26/50], Loss: 0.0569\n",
            "Accuracy of the model on the test images: 92.35 %\n",
            "Validation error of the model on the test images: 0.2105\n",
            "Epoch [27/50], Loss: 0.0704\n",
            "Accuracy of the model on the test images: 92.18 %\n",
            "Validation error of the model on the test images: 0.2102\n",
            "Epoch [28/50], Loss: 0.0700\n",
            "Accuracy of the model on the test images: 92.26 %\n",
            "Validation error of the model on the test images: 0.2066\n",
            "Epoch [29/50], Loss: 0.0636\n",
            "Epoch [30/50], Loss: 0.0665\n",
            "Epoch [31/50], Loss: 0.0678\n",
            "Accuracy of the model on the test images: 92.55 %\n",
            "Validation error of the model on the test images: 0.2011\n",
            "Epoch [32/50], Loss: 0.0780\n",
            "Accuracy of the model on the test images: 92.77 %\n",
            "Validation error of the model on the test images: 0.1938\n",
            "Epoch [33/50], Loss: 0.0769\n",
            "Epoch [34/50], Loss: 0.0628\n",
            "Accuracy of the model on the test images: 92.77 %\n",
            "Validation error of the model on the test images: 0.1910\n",
            "Epoch [35/50], Loss: 0.0641\n",
            "Accuracy of the model on the test images: 93.64 %\n",
            "Validation error of the model on the test images: 0.1692\n",
            "Epoch [36/50], Loss: 0.0767\n",
            "Epoch [37/50], Loss: 0.0471\n",
            "Epoch [38/50], Loss: 0.0647\n",
            "Epoch [39/50], Loss: 0.0779\n",
            "Accuracy of the model on the test images: 93.75 %\n",
            "Validation error of the model on the test images: 0.1660\n",
            "Epoch [40/50], Loss: 0.0633\n",
            "Epoch [41/50], Loss: 0.0557\n",
            "Epoch [42/50], Loss: 0.0821\n",
            "Epoch [43/50], Loss: 0.0575\n",
            "Accuracy of the model on the test images: 93.81 %\n",
            "Validation error of the model on the test images: 0.1602\n",
            "Epoch [44/50], Loss: 0.0622\n",
            "Accuracy of the model on the test images: 94.37 %\n",
            "Validation error of the model on the test images: 0.1527\n",
            "Epoch [45/50], Loss: 0.0609\n",
            "Accuracy of the model on the test images: 94.18 %\n",
            "Validation error of the model on the test images: 0.1507\n",
            "Epoch [46/50], Loss: 0.0721\n",
            "Epoch [47/50], Loss: 0.0627\n",
            "Epoch [48/50], Loss: 0.0512\n",
            "Epoch [49/50], Loss: 0.0601\n",
            "Accuracy of the model on the test images: 94.47 %\n",
            "Validation error of the model on the test images: 0.1480\n",
            "Epoch [50/50], Loss: 0.0578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gchurns_mixed, bchurns_mixed, accuracies_mixed = get_metrics(sets['mixed'].tensors[1], loaders['mixed'])\n",
        "# metrics_dict_multiple[0.5][\"gchurns_mixed\"] = gchurns_mixed\n",
        "# metrics_dict_multiple[0.5][\"bchurns_mixed\"] = bchurns_mixed\n",
        "# metrics_dict_multiple[0.5][\"accuracies_mixed\"] = accuracies_mixed\n",
        "\n",
        "gchurns_val, bchurns_val, accuracies_val = get_metrics(sets['val'].tensors[1], loaders['val'])\n",
        "metrics_dict_multiple[0.5][\"gchurns_val\"] = gchurns_val\n",
        "metrics_dict_multiple[0.5][\"bchurns_val\"] = bchurns_val\n",
        "metrics_dict_multiple[0.5][\"accuracies_val\"] = accuracies_val"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhlTbSq4niJa",
        "outputId": "9a416716-2303-428a-eb10-75b1646c7cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:20<00:00,  1.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lambda = 0.7"
      ],
      "metadata": {
        "id": "dJgjD-XCwpKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relabel based on threshold\n",
        "confidence_threshold = 1.0\n",
        "\n",
        "# gmm_labels = teacher_labels_mixed\n",
        "lamda = 0.7\n",
        "\n",
        "mixed_label = F.one_hot(sets['mixed'].tensors[1], num_classes=10)\n",
        "gmm_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * lamda + mixed_label * (1 - lamda)\n",
        "gmm_labels = torch.where(mixed_label != F.one_hot(y_mixed, num_classes=10), mixed_label, gmm_labels)\n",
        "\n",
        "sets[\"teacher_unlab\"] = torch.utils.data.TensorDataset(\n",
        "    total_dict['x'][np.isin(total_dict['field'], [\"unlab\"])],\n",
        "    unlab_teacherpred\n",
        ")\n",
        "\n",
        "sets[\"gmm\"] = torch.utils.data.TensorDataset(\n",
        "    torch.concat([\n",
        "        sets['mixed'].tensors[0],\n",
        "        sets['unlab'].tensors[0],\n",
        "    ]),\n",
        "    torch.concat([\n",
        "        gmm_labels,\n",
        "       unlab_teacherpred\n",
        "    ]),\n",
        ")\n",
        "loaders['gmm'] = torch.utils.data.DataLoader(sets[\"gmm\"], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "train_dist(\n",
        "    name=\"student_unlab_model\", \n",
        "    loader=loaders['gmm'],\n",
        ")\n",
        "student_gmm_model = CNN()\n",
        "student_gmm_model.load_state_dict(torch.load(\"student_unlab_model.pth\"))\n",
        "student_gmm_model.eval()\n",
        "None"
      ],
      "metadata": {
        "id": "sMR15T8LGGA-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "158cf99a-05fe-46c1-baa0-903cfcf20de0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5537\n",
            "Accuracy of the model on the test images: 83.26 %\n",
            "Validation error of the model on the test images: 0.4799\n",
            "Epoch [2/50], Loss: 0.4857\n",
            "Accuracy of the model on the test images: 85.80 %\n",
            "Validation error of the model on the test images: 0.4076\n",
            "Epoch [3/50], Loss: 0.4078\n",
            "Accuracy of the model on the test images: 87.64 %\n",
            "Validation error of the model on the test images: 0.3587\n",
            "Epoch [4/50], Loss: 0.3881\n",
            "Accuracy of the model on the test images: 88.56 %\n",
            "Validation error of the model on the test images: 0.3350\n",
            "Epoch [5/50], Loss: 0.3491\n",
            "Accuracy of the model on the test images: 89.06 %\n",
            "Validation error of the model on the test images: 0.3212\n",
            "Epoch [6/50], Loss: 0.3437\n",
            "Accuracy of the model on the test images: 89.65 %\n",
            "Validation error of the model on the test images: 0.2980\n",
            "Epoch [7/50], Loss: 0.3105\n",
            "Accuracy of the model on the test images: 90.36 %\n",
            "Validation error of the model on the test images: 0.2822\n",
            "Epoch [8/50], Loss: 0.3151\n",
            "Accuracy of the model on the test images: 90.82 %\n",
            "Validation error of the model on the test images: 0.2684\n",
            "Epoch [9/50], Loss: 0.2968\n",
            "Accuracy of the model on the test images: 91.17 %\n",
            "Validation error of the model on the test images: 0.2602\n",
            "Epoch [10/50], Loss: 0.2764\n",
            "Accuracy of the model on the test images: 91.32 %\n",
            "Validation error of the model on the test images: 0.2531\n",
            "Epoch [11/50], Loss: 0.2813\n",
            "Accuracy of the model on the test images: 91.70 %\n",
            "Validation error of the model on the test images: 0.2441\n",
            "Epoch [12/50], Loss: 0.2853\n",
            "Accuracy of the model on the test images: 91.97 %\n",
            "Validation error of the model on the test images: 0.2360\n",
            "Epoch [13/50], Loss: 0.2621\n",
            "Accuracy of the model on the test images: 92.08 %\n",
            "Validation error of the model on the test images: 0.2296\n",
            "Epoch [14/50], Loss: 0.2890\n",
            "Accuracy of the model on the test images: 92.40 %\n",
            "Validation error of the model on the test images: 0.2241\n",
            "Epoch [15/50], Loss: 0.2807\n",
            "Accuracy of the model on the test images: 92.84 %\n",
            "Validation error of the model on the test images: 0.2143\n",
            "Epoch [16/50], Loss: 0.2969\n",
            "Accuracy of the model on the test images: 92.99 %\n",
            "Validation error of the model on the test images: 0.2082\n",
            "Epoch [17/50], Loss: 0.2630\n",
            "Accuracy of the model on the test images: 93.19 %\n",
            "Validation error of the model on the test images: 0.2031\n",
            "Epoch [18/50], Loss: 0.2673\n",
            "Accuracy of the model on the test images: 93.39 %\n",
            "Validation error of the model on the test images: 0.1985\n",
            "Epoch [19/50], Loss: 0.2711\n",
            "Accuracy of the model on the test images: 93.75 %\n",
            "Validation error of the model on the test images: 0.1910\n",
            "Epoch [20/50], Loss: 0.2696\n",
            "Epoch [21/50], Loss: 0.2619\n",
            "Accuracy of the model on the test images: 93.80 %\n",
            "Validation error of the model on the test images: 0.1884\n",
            "Epoch [22/50], Loss: 0.2634\n",
            "Accuracy of the model on the test images: 94.05 %\n",
            "Validation error of the model on the test images: 0.1842\n",
            "Epoch [23/50], Loss: 0.2832\n",
            "Accuracy of the model on the test images: 94.19 %\n",
            "Validation error of the model on the test images: 0.1777\n",
            "Epoch [24/50], Loss: 0.2318\n",
            "Accuracy of the model on the test images: 94.29 %\n",
            "Validation error of the model on the test images: 0.1765\n",
            "Epoch [25/50], Loss: 0.2679\n",
            "Epoch [26/50], Loss: 0.2427\n",
            "Accuracy of the model on the test images: 94.49 %\n",
            "Validation error of the model on the test images: 0.1725\n",
            "Epoch [27/50], Loss: 0.2652\n",
            "Epoch [28/50], Loss: 0.2527\n",
            "Accuracy of the model on the test images: 94.89 %\n",
            "Validation error of the model on the test images: 0.1677\n",
            "Epoch [29/50], Loss: 0.2331\n",
            "Accuracy of the model on the test images: 94.95 %\n",
            "Validation error of the model on the test images: 0.1626\n",
            "Epoch [30/50], Loss: 0.2572\n",
            "Epoch [31/50], Loss: 0.2592\n",
            "Epoch [32/50], Loss: 0.2237\n",
            "Accuracy of the model on the test images: 95.02 %\n",
            "Validation error of the model on the test images: 0.1601\n",
            "Epoch [33/50], Loss: 0.2364\n",
            "Accuracy of the model on the test images: 95.06 %\n",
            "Validation error of the model on the test images: 0.1593\n",
            "Epoch [34/50], Loss: 0.2650\n",
            "Accuracy of the model on the test images: 95.11 %\n",
            "Validation error of the model on the test images: 0.1559\n",
            "Epoch [35/50], Loss: 0.2356\n",
            "Epoch [36/50], Loss: 0.2410\n",
            "Epoch [37/50], Loss: 0.2436\n",
            "Accuracy of the model on the test images: 95.28 %\n",
            "Validation error of the model on the test images: 0.1547\n",
            "Epoch [38/50], Loss: 0.2614\n",
            "Accuracy of the model on the test images: 95.35 %\n",
            "Validation error of the model on the test images: 0.1538\n",
            "Epoch [39/50], Loss: 0.2292\n",
            "Accuracy of the model on the test images: 95.43 %\n",
            "Validation error of the model on the test images: 0.1492\n",
            "Epoch [40/50], Loss: 0.2275\n",
            "Epoch [41/50], Loss: 0.2380\n",
            "Accuracy of the model on the test images: 95.66 %\n",
            "Validation error of the model on the test images: 0.1470\n",
            "Epoch [42/50], Loss: 0.2470\n",
            "Accuracy of the model on the test images: 95.66 %\n",
            "Validation error of the model on the test images: 0.1463\n",
            "Epoch [43/50], Loss: 0.2389\n",
            "Accuracy of the model on the test images: 95.70 %\n",
            "Validation error of the model on the test images: 0.1449\n",
            "Epoch [44/50], Loss: 0.2460\n",
            "Accuracy of the model on the test images: 95.83 %\n",
            "Validation error of the model on the test images: 0.1446\n",
            "Epoch [45/50], Loss: 0.2539\n",
            "Accuracy of the model on the test images: 95.81 %\n",
            "Validation error of the model on the test images: 0.1431\n",
            "Epoch [46/50], Loss: 0.2324\n",
            "Epoch [47/50], Loss: 0.2543\n",
            "Accuracy of the model on the test images: 95.89 %\n",
            "Validation error of the model on the test images: 0.1407\n",
            "Epoch [48/50], Loss: 0.2419\n",
            "Accuracy of the model on the test images: 95.95 %\n",
            "Validation error of the model on the test images: 0.1397\n",
            "Epoch [49/50], Loss: 0.2392\n",
            "Accuracy of the model on the test images: 95.99 %\n",
            "Validation error of the model on the test images: 0.1375\n",
            "Epoch [50/50], Loss: 0.2613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lamda = 0.7\n",
        "distill_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * (lamda) + F.one_hot(sets['mixed'].tensors[1], num_classes=10) * (1-lamda)\n",
        "\n",
        "sets[\"distill\"] = torch.utils.data.TensorDataset(\n",
        "    sets['mixed'].tensors[0],\n",
        "    distill_labels,\n",
        ")\n",
        "\n",
        "loaders['distill'] = torch.utils.data.DataLoader(sets['distill'], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loader=loaders['distill']\n",
        "\n",
        "train_dist(\n",
        "    name=\"student_anchor_model\", \n",
        "    loader=loaders['distill'],\n",
        ")\n",
        "student_anchor_model = CNN()\n",
        "student_anchor_model.load_state_dict(torch.load(\"student_anchor_model.pth\"))\n",
        "student_anchor_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23tniO7Sw3sT",
        "outputId": "552143d0-b460-48e6-9a93-edca468c298e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.1357\n",
            "Accuracy of the model on the test images: 70.05 %\n",
            "Validation error of the model on the test images: 1.0321\n",
            "Epoch [2/50], Loss: 0.1412\n",
            "Accuracy of the model on the test images: 75.95 %\n",
            "Validation error of the model on the test images: 0.7811\n",
            "Epoch [3/50], Loss: 0.1407\n",
            "Accuracy of the model on the test images: 81.45 %\n",
            "Validation error of the model on the test images: 0.5562\n",
            "Epoch [4/50], Loss: 0.0880\n",
            "Accuracy of the model on the test images: 82.50 %\n",
            "Validation error of the model on the test images: 0.5467\n",
            "Epoch [5/50], Loss: 0.1100\n",
            "Accuracy of the model on the test images: 84.35 %\n",
            "Validation error of the model on the test images: 0.4826\n",
            "Epoch [6/50], Loss: 0.0968\n",
            "Accuracy of the model on the test images: 85.29 %\n",
            "Validation error of the model on the test images: 0.4344\n",
            "Epoch [7/50], Loss: 0.1001\n",
            "Epoch [8/50], Loss: 0.0720\n",
            "Accuracy of the model on the test images: 86.73 %\n",
            "Validation error of the model on the test images: 0.3932\n",
            "Epoch [9/50], Loss: 0.1096\n",
            "Accuracy of the model on the test images: 87.19 %\n",
            "Validation error of the model on the test images: 0.3913\n",
            "Epoch [10/50], Loss: 0.1123\n",
            "Accuracy of the model on the test images: 88.75 %\n",
            "Validation error of the model on the test images: 0.3281\n",
            "Epoch [11/50], Loss: 0.0999\n",
            "Accuracy of the model on the test images: 88.71 %\n",
            "Validation error of the model on the test images: 0.3235\n",
            "Epoch [12/50], Loss: 0.1000\n",
            "Accuracy of the model on the test images: 89.56 %\n",
            "Validation error of the model on the test images: 0.3084\n",
            "Epoch [13/50], Loss: 0.0744\n",
            "Accuracy of the model on the test images: 89.88 %\n",
            "Validation error of the model on the test images: 0.2878\n",
            "Epoch [14/50], Loss: 0.0694\n",
            "Accuracy of the model on the test images: 90.55 %\n",
            "Validation error of the model on the test images: 0.2672\n",
            "Epoch [15/50], Loss: 0.0606\n",
            "Accuracy of the model on the test images: 90.53 %\n",
            "Validation error of the model on the test images: 0.2658\n",
            "Epoch [16/50], Loss: 0.0904\n",
            "Accuracy of the model on the test images: 90.93 %\n",
            "Validation error of the model on the test images: 0.2584\n",
            "Epoch [17/50], Loss: 0.0770\n",
            "Accuracy of the model on the test images: 91.59 %\n",
            "Validation error of the model on the test images: 0.2357\n",
            "Epoch [18/50], Loss: 0.0513\n",
            "Epoch [19/50], Loss: 0.0683\n",
            "Epoch [20/50], Loss: 0.0795\n",
            "Epoch [21/50], Loss: 0.0815\n",
            "Accuracy of the model on the test images: 91.97 %\n",
            "Validation error of the model on the test images: 0.2283\n",
            "Epoch [22/50], Loss: 0.0790\n",
            "Accuracy of the model on the test images: 92.71 %\n",
            "Validation error of the model on the test images: 0.2089\n",
            "Epoch [23/50], Loss: 0.0823\n",
            "Epoch [24/50], Loss: 0.0598\n",
            "Accuracy of the model on the test images: 92.63 %\n",
            "Validation error of the model on the test images: 0.2037\n",
            "Epoch [25/50], Loss: 0.0646\n",
            "Accuracy of the model on the test images: 92.86 %\n",
            "Validation error of the model on the test images: 0.2020\n",
            "Epoch [26/50], Loss: 0.0639\n",
            "Epoch [27/50], Loss: 0.0714\n",
            "Accuracy of the model on the test images: 93.19 %\n",
            "Validation error of the model on the test images: 0.1916\n",
            "Epoch [28/50], Loss: 0.0558\n",
            "Accuracy of the model on the test images: 93.31 %\n",
            "Validation error of the model on the test images: 0.1895\n",
            "Epoch [29/50], Loss: 0.0611\n",
            "Epoch [30/50], Loss: 0.0652\n",
            "Epoch [31/50], Loss: 0.0580\n",
            "Epoch [32/50], Loss: 0.0860\n",
            "Accuracy of the model on the test images: 93.76 %\n",
            "Validation error of the model on the test images: 0.1798\n",
            "Epoch [33/50], Loss: 0.0758\n",
            "Epoch [34/50], Loss: 0.0525\n",
            "Epoch [35/50], Loss: 0.0661\n",
            "Accuracy of the model on the test images: 94.59 %\n",
            "Validation error of the model on the test images: 0.1559\n",
            "Epoch [36/50], Loss: 0.0697\n",
            "Epoch [37/50], Loss: 0.0426\n",
            "Epoch [38/50], Loss: 0.0515\n",
            "Epoch [39/50], Loss: 0.0659\n",
            "Accuracy of the model on the test images: 94.59 %\n",
            "Validation error of the model on the test images: 0.1559\n",
            "Epoch [40/50], Loss: 0.0640\n",
            "Epoch [41/50], Loss: 0.0596\n",
            "Epoch [42/50], Loss: 0.0782\n",
            "Accuracy of the model on the test images: 94.97 %\n",
            "Validation error of the model on the test images: 0.1461\n",
            "Epoch [43/50], Loss: 0.0769\n",
            "Epoch [44/50], Loss: 0.0695\n",
            "Accuracy of the model on the test images: 95.11 %\n",
            "Validation error of the model on the test images: 0.1384\n",
            "Epoch [45/50], Loss: 0.0842\n",
            "Epoch [46/50], Loss: 0.0624\n",
            "Accuracy of the model on the test images: 95.37 %\n",
            "Validation error of the model on the test images: 0.1347\n",
            "Epoch [47/50], Loss: 0.0694\n",
            "Accuracy of the model on the test images: 95.38 %\n",
            "Validation error of the model on the test images: 0.1342\n",
            "Epoch [48/50], Loss: 0.0633\n",
            "Epoch [49/50], Loss: 0.0749\n",
            "Epoch [50/50], Loss: 0.0611\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gchurns_mixed, bchurns_mixed, accuracies_mixed = get_metrics(sets['mixed'].tensors[1], loaders['mixed'])\n",
        "# metrics_dict_multiple[0.7][\"gchurns_mixed\"] = gchurns_mixed\n",
        "# metrics_dict_multiple[0.7][\"bchurns_mixed\"] = bchurns_mixed\n",
        "# metrics_dict_multiple[0.7][\"accuracies_mixed\"] = accuracies_mixed\n",
        "\n",
        "gchurns_val, bchurns_val, accuracies_val = get_metrics(sets['val'].tensors[1], loaders['val'])\n",
        "metrics_dict_multiple[0.7][\"gchurns_val\"] = gchurns_val\n",
        "metrics_dict_multiple[0.7][\"bchurns_val\"] = bchurns_val\n",
        "metrics_dict_multiple[0.7][\"accuracies_val\"] = accuracies_val\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsgp8-F2xO90",
        "outputId": "a4263f16-9c3a-49ec-8761-3e124771a2e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:21<00:00,  1.50it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lambda = 0.9"
      ],
      "metadata": {
        "id": "0gV5DcRAw3WN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relabel based on threshold\n",
        "confidence_threshold = 1.0\n",
        "\n",
        "# gmm_labels = teacher_labels_mixed\n",
        "lamda = 0.9\n",
        "\n",
        "mixed_label = F.one_hot(sets['mixed'].tensors[1], num_classes=10)\n",
        "gmm_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * lamda + mixed_label * (1 - lamda)\n",
        "gmm_labels = torch.where(mixed_label != F.one_hot(y_mixed, num_classes=10), mixed_label, gmm_labels)\n",
        "\n",
        "sets[\"teacher_unlab\"] = torch.utils.data.TensorDataset(\n",
        "    total_dict['x'][np.isin(total_dict['field'], [\"unlab\"])],\n",
        "    unlab_teacherpred\n",
        ")\n",
        "\n",
        "sets[\"gmm\"] = torch.utils.data.TensorDataset(\n",
        "    torch.concat([\n",
        "        sets['mixed'].tensors[0],\n",
        "        sets['unlab'].tensors[0],\n",
        "    ]),\n",
        "    torch.concat([\n",
        "        gmm_labels,\n",
        "       unlab_teacherpred\n",
        "    ]),\n",
        ")\n",
        "loaders['gmm'] = torch.utils.data.DataLoader(sets[\"gmm\"], batch_size=batch_size, shuffle=False)\n",
        "train_dist(\n",
        "    name=\"student_unlab_model\", \n",
        "    loader=loaders['gmm'],\n",
        ")\n",
        "student_gmm_model = CNN()\n",
        "student_gmm_model.load_state_dict(torch.load(\"student_unlab_model.pth\"))\n",
        "student_gmm_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c0WOXNGw6CR",
        "outputId": "92b62c16-f734-4b91-ee6b-ed2ed5a3c875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.5557\n",
            "Accuracy of the model on the test images: 83.65 %\n",
            "Validation error of the model on the test images: 0.4680\n",
            "Epoch [2/50], Loss: 0.4719\n",
            "Accuracy of the model on the test images: 86.54 %\n",
            "Validation error of the model on the test images: 0.3836\n",
            "Epoch [3/50], Loss: 0.4135\n",
            "Accuracy of the model on the test images: 87.81 %\n",
            "Validation error of the model on the test images: 0.3488\n",
            "Epoch [4/50], Loss: 0.3974\n",
            "Accuracy of the model on the test images: 88.68 %\n",
            "Validation error of the model on the test images: 0.3236\n",
            "Epoch [5/50], Loss: 0.3414\n",
            "Accuracy of the model on the test images: 89.27 %\n",
            "Validation error of the model on the test images: 0.3049\n",
            "Epoch [6/50], Loss: 0.3322\n",
            "Accuracy of the model on the test images: 89.50 %\n",
            "Validation error of the model on the test images: 0.2940\n",
            "Epoch [7/50], Loss: 0.3222\n",
            "Accuracy of the model on the test images: 90.88 %\n",
            "Validation error of the model on the test images: 0.2605\n",
            "Epoch [8/50], Loss: 0.3313\n",
            "Accuracy of the model on the test images: 91.13 %\n",
            "Validation error of the model on the test images: 0.2539\n",
            "Epoch [9/50], Loss: 0.3085\n",
            "Accuracy of the model on the test images: 91.18 %\n",
            "Validation error of the model on the test images: 0.2508\n",
            "Epoch [10/50], Loss: 0.2756\n",
            "Accuracy of the model on the test images: 91.53 %\n",
            "Validation error of the model on the test images: 0.2411\n",
            "Epoch [11/50], Loss: 0.2946\n",
            "Accuracy of the model on the test images: 91.97 %\n",
            "Validation error of the model on the test images: 0.2281\n",
            "Epoch [12/50], Loss: 0.2949\n",
            "Accuracy of the model on the test images: 92.31 %\n",
            "Validation error of the model on the test images: 0.2226\n",
            "Epoch [13/50], Loss: 0.2682\n",
            "Accuracy of the model on the test images: 92.34 %\n",
            "Validation error of the model on the test images: 0.2176\n",
            "Epoch [14/50], Loss: 0.2917\n",
            "Accuracy of the model on the test images: 92.65 %\n",
            "Validation error of the model on the test images: 0.2117\n",
            "Epoch [15/50], Loss: 0.2589\n",
            "Accuracy of the model on the test images: 93.16 %\n",
            "Validation error of the model on the test images: 0.2002\n",
            "Epoch [16/50], Loss: 0.2616\n",
            "Accuracy of the model on the test images: 93.33 %\n",
            "Validation error of the model on the test images: 0.1963\n",
            "Epoch [17/50], Loss: 0.2617\n",
            "Accuracy of the model on the test images: 93.52 %\n",
            "Validation error of the model on the test images: 0.1883\n",
            "Epoch [18/50], Loss: 0.2538\n",
            "Epoch [19/50], Loss: 0.2580\n",
            "Accuracy of the model on the test images: 93.79 %\n",
            "Validation error of the model on the test images: 0.1810\n",
            "Epoch [20/50], Loss: 0.2563\n",
            "Accuracy of the model on the test images: 93.93 %\n",
            "Validation error of the model on the test images: 0.1791\n",
            "Epoch [21/50], Loss: 0.2660\n",
            "Accuracy of the model on the test images: 93.98 %\n",
            "Validation error of the model on the test images: 0.1762\n",
            "Epoch [22/50], Loss: 0.2633\n",
            "Accuracy of the model on the test images: 94.32 %\n",
            "Validation error of the model on the test images: 0.1707\n",
            "Epoch [23/50], Loss: 0.2605\n",
            "Accuracy of the model on the test images: 94.72 %\n",
            "Validation error of the model on the test images: 0.1638\n",
            "Epoch [24/50], Loss: 0.2417\n",
            "Epoch [25/50], Loss: 0.2574\n",
            "Epoch [26/50], Loss: 0.2382\n",
            "Epoch [27/50], Loss: 0.2505\n",
            "Accuracy of the model on the test images: 94.71 %\n",
            "Validation error of the model on the test images: 0.1613\n",
            "Epoch [28/50], Loss: 0.2597\n",
            "Accuracy of the model on the test images: 94.88 %\n",
            "Validation error of the model on the test images: 0.1588\n",
            "Epoch [29/50], Loss: 0.2537\n",
            "Accuracy of the model on the test images: 95.17 %\n",
            "Validation error of the model on the test images: 0.1527\n",
            "Epoch [30/50], Loss: 0.2345\n",
            "Epoch [31/50], Loss: 0.2446\n",
            "Epoch [32/50], Loss: 0.2216\n",
            "Epoch [33/50], Loss: 0.2334\n",
            "Accuracy of the model on the test images: 95.31 %\n",
            "Validation error of the model on the test images: 0.1498\n",
            "Epoch [34/50], Loss: 0.2543\n",
            "Accuracy of the model on the test images: 95.36 %\n",
            "Validation error of the model on the test images: 0.1437\n",
            "Epoch [35/50], Loss: 0.2407\n",
            "Epoch [36/50], Loss: 0.2253\n",
            "Epoch [37/50], Loss: 0.2299\n",
            "Accuracy of the model on the test images: 95.58 %\n",
            "Validation error of the model on the test images: 0.1424\n",
            "Epoch [38/50], Loss: 0.2701\n",
            "Epoch [39/50], Loss: 0.2347\n",
            "Accuracy of the model on the test images: 95.62 %\n",
            "Validation error of the model on the test images: 0.1421\n",
            "Epoch [40/50], Loss: 0.2330\n",
            "Epoch [41/50], Loss: 0.2406\n",
            "Accuracy of the model on the test images: 95.79 %\n",
            "Validation error of the model on the test images: 0.1373\n",
            "Epoch [42/50], Loss: 0.2480\n",
            "Accuracy of the model on the test images: 95.94 %\n",
            "Validation error of the model on the test images: 0.1359\n",
            "Epoch [43/50], Loss: 0.2581\n",
            "Accuracy of the model on the test images: 95.93 %\n",
            "Validation error of the model on the test images: 0.1346\n",
            "Epoch [44/50], Loss: 0.2341\n",
            "Epoch [45/50], Loss: 0.2570\n",
            "Accuracy of the model on the test images: 96.06 %\n",
            "Validation error of the model on the test images: 0.1333\n",
            "Epoch [46/50], Loss: 0.2245\n",
            "Accuracy of the model on the test images: 96.07 %\n",
            "Validation error of the model on the test images: 0.1320\n",
            "Epoch [47/50], Loss: 0.2429\n",
            "Accuracy of the model on the test images: 96.18 %\n",
            "Validation error of the model on the test images: 0.1289\n",
            "Epoch [48/50], Loss: 0.2404\n",
            "Epoch [49/50], Loss: 0.2397\n",
            "Epoch [50/50], Loss: 0.2522\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lamda = 0.9\n",
        "distill_labels = F.one_hot(teacher_labels_mixed, num_classes=10) * (lamda) + F.one_hot(sets['mixed'].tensors[1], num_classes=10) * (1-lamda)\n",
        "\n",
        "sets[\"distill\"] = torch.utils.data.TensorDataset(\n",
        "    sets['mixed'].tensors[0],\n",
        "    distill_labels,\n",
        ")\n",
        "\n",
        "loaders['distill'] = torch.utils.data.DataLoader(sets['distill'], batch_size=batch_size, shuffle=False)\n",
        "\n",
        "loader=loaders['distill']\n",
        "\n",
        "train_dist(\n",
        "    name=\"student_anchor_model\", \n",
        "    loader=loaders['distill'],\n",
        ")\n",
        "student_anchor_model = CNN()\n",
        "student_anchor_model.load_state_dict(torch.load(\"student_anchor_model.pth\"))\n",
        "student_anchor_model.eval()\n",
        "None"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlL9SrnMxHfn",
        "outputId": "6a829ba2-1e7d-4789-a1ee-a5a7771aa031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50], Loss: 0.1414\n",
            "Accuracy of the model on the test images: 74.69 %\n",
            "Validation error of the model on the test images: 0.8081\n",
            "Epoch [2/50], Loss: 0.1426\n",
            "Accuracy of the model on the test images: 79.48 %\n",
            "Validation error of the model on the test images: 0.6259\n",
            "Epoch [3/50], Loss: 0.1368\n",
            "Accuracy of the model on the test images: 81.47 %\n",
            "Validation error of the model on the test images: 0.5295\n",
            "Epoch [4/50], Loss: 0.1068\n",
            "Accuracy of the model on the test images: 83.43 %\n",
            "Validation error of the model on the test images: 0.4934\n",
            "Epoch [5/50], Loss: 0.0983\n",
            "Accuracy of the model on the test images: 85.26 %\n",
            "Validation error of the model on the test images: 0.4410\n",
            "Epoch [6/50], Loss: 0.0920\n",
            "Accuracy of the model on the test images: 87.09 %\n",
            "Validation error of the model on the test images: 0.3688\n",
            "Epoch [7/50], Loss: 0.1161\n",
            "Epoch [8/50], Loss: 0.0700\n",
            "Accuracy of the model on the test images: 88.49 %\n",
            "Validation error of the model on the test images: 0.3324\n",
            "Epoch [9/50], Loss: 0.0983\n",
            "Epoch [10/50], Loss: 0.1121\n",
            "Accuracy of the model on the test images: 89.22 %\n",
            "Validation error of the model on the test images: 0.3089\n",
            "Epoch [11/50], Loss: 0.0901\n",
            "Accuracy of the model on the test images: 89.27 %\n",
            "Validation error of the model on the test images: 0.3002\n",
            "Epoch [12/50], Loss: 0.0878\n",
            "Accuracy of the model on the test images: 90.36 %\n",
            "Validation error of the model on the test images: 0.2731\n",
            "Epoch [13/50], Loss: 0.0600\n",
            "Accuracy of the model on the test images: 90.66 %\n",
            "Validation error of the model on the test images: 0.2572\n",
            "Epoch [14/50], Loss: 0.0748\n",
            "Accuracy of the model on the test images: 90.70 %\n",
            "Validation error of the model on the test images: 0.2567\n",
            "Epoch [15/50], Loss: 0.0594\n",
            "Accuracy of the model on the test images: 90.90 %\n",
            "Validation error of the model on the test images: 0.2504\n",
            "Epoch [16/50], Loss: 0.0807\n",
            "Accuracy of the model on the test images: 91.37 %\n",
            "Validation error of the model on the test images: 0.2364\n",
            "Epoch [17/50], Loss: 0.0885\n",
            "Accuracy of the model on the test images: 92.20 %\n",
            "Validation error of the model on the test images: 0.2113\n",
            "Epoch [18/50], Loss: 0.0385\n",
            "Epoch [19/50], Loss: 0.0643\n",
            "Accuracy of the model on the test images: 92.39 %\n",
            "Validation error of the model on the test images: 0.2061\n",
            "Epoch [20/50], Loss: 0.0644\n",
            "Epoch [21/50], Loss: 0.0628\n",
            "Accuracy of the model on the test images: 92.69 %\n",
            "Validation error of the model on the test images: 0.1973\n",
            "Epoch [22/50], Loss: 0.0716\n",
            "Epoch [23/50], Loss: 0.0849\n",
            "Accuracy of the model on the test images: 92.95 %\n",
            "Validation error of the model on the test images: 0.1914\n",
            "Epoch [24/50], Loss: 0.0517\n",
            "Accuracy of the model on the test images: 93.31 %\n",
            "Validation error of the model on the test images: 0.1814\n",
            "Epoch [25/50], Loss: 0.0534\n",
            "Accuracy of the model on the test images: 93.63 %\n",
            "Validation error of the model on the test images: 0.1746\n",
            "Epoch [26/50], Loss: 0.0365\n",
            "Epoch [27/50], Loss: 0.0636\n",
            "Accuracy of the model on the test images: 93.87 %\n",
            "Validation error of the model on the test images: 0.1683\n",
            "Epoch [28/50], Loss: 0.0407\n",
            "Epoch [29/50], Loss: 0.0611\n",
            "Epoch [30/50], Loss: 0.0600\n",
            "Epoch [31/50], Loss: 0.0483\n",
            "Accuracy of the model on the test images: 94.15 %\n",
            "Validation error of the model on the test images: 0.1591\n",
            "Epoch [32/50], Loss: 0.0962\n",
            "Accuracy of the model on the test images: 94.44 %\n",
            "Validation error of the model on the test images: 0.1556\n",
            "Epoch [33/50], Loss: 0.0547\n",
            "Accuracy of the model on the test images: 94.19 %\n",
            "Validation error of the model on the test images: 0.1552\n",
            "Epoch [34/50], Loss: 0.0455\n",
            "Accuracy of the model on the test images: 94.84 %\n",
            "Validation error of the model on the test images: 0.1432\n",
            "Epoch [35/50], Loss: 0.0398\n",
            "Accuracy of the model on the test images: 94.86 %\n",
            "Validation error of the model on the test images: 0.1388\n",
            "Epoch [36/50], Loss: 0.0529\n",
            "Epoch [37/50], Loss: 0.0337\n",
            "Epoch [38/50], Loss: 0.0437\n",
            "Epoch [39/50], Loss: 0.0547\n",
            "Epoch [40/50], Loss: 0.0401\n",
            "Accuracy of the model on the test images: 95.15 %\n",
            "Validation error of the model on the test images: 0.1299\n",
            "Epoch [41/50], Loss: 0.0465\n",
            "Epoch [42/50], Loss: 0.0496\n",
            "Epoch [43/50], Loss: 0.0403\n",
            "Epoch [44/50], Loss: 0.0499\n",
            "Accuracy of the model on the test images: 95.57 %\n",
            "Validation error of the model on the test images: 0.1223\n",
            "Epoch [45/50], Loss: 0.0597\n",
            "Accuracy of the model on the test images: 95.78 %\n",
            "Validation error of the model on the test images: 0.1159\n",
            "Epoch [46/50], Loss: 0.0616\n",
            "Epoch [47/50], Loss: 0.0467\n",
            "Accuracy of the model on the test images: 95.84 %\n",
            "Validation error of the model on the test images: 0.1144\n",
            "Epoch [48/50], Loss: 0.0257\n",
            "Epoch [49/50], Loss: 0.0496\n",
            "Epoch [50/50], Loss: 0.0457\n",
            "Accuracy of the model on the test images: 96.01 %\n",
            "Validation error of the model on the test images: 0.1096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gchurns_mixed, bchurns_mixed, accuracies_mixed = get_metrics(sets['mixed'].tensors[1], loaders['mixed'])\n",
        "# metrics_dict_multiple[0.9][\"gchurns_mixed\"] = gchurns_mixed\n",
        "# metrics_dict_multiple[0.9][\"bchurns_mixed\"] = bchurns_mixed\n",
        "# metrics_dict_multiple[0.9][\"accuracies_mixed\"] = accuracies_mixed\n",
        "\n",
        "gchurns_val, bchurns_val, accuracies_val = get_metrics(sets['val'].tensors[1], loaders['val'])\n",
        "metrics_dict_multiple[0.9][\"gchurns_val\"] = gchurns_val\n",
        "metrics_dict_multiple[0.9][\"bchurns_val\"] = bchurns_val\n",
        "metrics_dict_multiple[0.9][\"accuracies_val\"] = accuracies_val\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgxqE2prxo3v",
        "outputId": "f554899d-e6ff-46a4-9a2c-b91caf5336c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 32/32 [00:19<00:00,  1.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_dict_multiple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpElg2w1YEoo",
        "outputId": "863fcaca-ef1b-4105-96cb-ee9fb184f76e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0.0: {'gchurns_val': {'baseline': 176, 'add_unlab': 124, 'anchor': 170},\n",
              "  'bchurns_val': {'baseline': 286, 'add_unlab': 221, 'anchor': 299},\n",
              "  'accuracies_val': {'baseline': 91.4749984741211,\n",
              "   'add_unlab': 91.4124984741211,\n",
              "   'anchor': 91.2125015258789,\n",
              "   'teacher': 92.0250015258789}},\n",
              " 0.3: {'gchurns_val': {'baseline': 176, 'add_unlab': 106, 'anchor': 160},\n",
              "  'bchurns_val': {'baseline': 286, 'add_unlab': 216, 'anchor': 254},\n",
              "  'accuracies_val': {'baseline': 91.4749984741211,\n",
              "   'add_unlab': 91.30000305175781,\n",
              "   'anchor': 91.6500015258789,\n",
              "   'teacher': 92.0250015258789}},\n",
              " 0.5: {'gchurns_val': {'baseline': 176, 'add_unlab': 113, 'anchor': 141},\n",
              "  'bchurns_val': {'baseline': 286, 'add_unlab': 215, 'anchor': 248},\n",
              "  'accuracies_val': {'baseline': 91.4749984741211,\n",
              "   'add_unlab': 91.32499694824219,\n",
              "   'anchor': 91.51249694824219,\n",
              "   'teacher': 92.0250015258789}},\n",
              " 0.7: {'gchurns_val': {'baseline': 176, 'add_unlab': 108, 'anchor': 138},\n",
              "  'bchurns_val': {'baseline': 286, 'add_unlab': 192, 'anchor': 237},\n",
              "  'accuracies_val': {'baseline': 91.4749984741211,\n",
              "   'add_unlab': 91.55000305175781,\n",
              "   'anchor': 91.5374984741211,\n",
              "   'teacher': 92.0250015258789}},\n",
              " 0.9: {'gchurns_val': {'baseline': 176, 'add_unlab': 112, 'anchor': 133},\n",
              "  'bchurns_val': {'baseline': 286, 'add_unlab': 175, 'anchor': 221},\n",
              "  'accuracies_val': {'baseline': 91.4749984741211,\n",
              "   'add_unlab': 91.7750015258789,\n",
              "   'anchor': 91.6500015258789,\n",
              "   'teacher': 92.0250015258789}}}"
            ]
          },
          "metadata": {},
          "execution_count": 280
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "hyperparams = [0.0, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "# plot accuracies for each model\n",
        "for model in ['baseline', 'add_unlab', 'anchor']:\n",
        "    model_accuracies = []\n",
        "    for hyperparam in hyperparams:\n",
        "        model_accuracies.append(metrics_dict_multiple[hyperparam]['gchurns_val'][model])\n",
        "    plt.plot(hyperparams, model_accuracies, label=model)\n",
        "\n",
        "plt.title('Good Churn for different models')\n",
        "plt.xlabel('Lambda value')\n",
        "plt.ylabel('Good Churn')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "5Yf8bb_C01YN",
        "outputId": "a42d6b24-1dac-4a08-c5c8-01f07e35dc1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "hyperparams = [0.0, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "# plot accuracies for each model\n",
        "for model in ['baseline', 'add_unlab', 'anchor']:\n",
        "    model_accuracies = []\n",
        "    for hyperparam in hyperparams:\n",
        "        model_accuracies.append(metrics_dict_multiple[hyperparam]['bchurns_val'][model])\n",
        "    plt.plot(hyperparams, model_accuracies, label=model)\n",
        "\n",
        "plt.title('Bad Churn for different models')\n",
        "plt.xlabel('Lambda value')\n",
        "plt.ylabel('Bad Churn')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "qPyreUEM1QC1",
        "outputId": "bba31bbd-5e00-4d99-c0cc-e22267c63ee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparams = [0.0, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "# plot accuracies for each model\n",
        "for model in ['baseline', 'add_unlab', 'anchor']:\n",
        "    model_accuracies = []\n",
        "    for hyperparam in hyperparams:\n",
        "        model_accuracies.append(metrics_dict_multiple[hyperparam]['accuracies_val'][model])\n",
        "    plt.plot(hyperparams, model_accuracies, label=model)\n",
        "\n",
        "plt.title('Bad Churn for different models')\n",
        "plt.xlabel('Lambda value')\n",
        "plt.ylabel('Bad Churn')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "w0I_PDMR2go2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "c2f174ae-6622-4a19-b1b1-4961a84062dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# create the pandas dataframe\n",
        "df = pd.DataFrame.from_dict({(i,j): metrics_dict_multiple[i][j] \n",
        "                             for i in metrics_dict_multiple.keys() \n",
        "                             for j in metrics_dict_multiple[i].keys()},\n",
        "                            orient='index')\n",
        "\n",
        "# save the dataframe as a CSV file\n",
        "df.to_csv('result_46.csv')"
      ],
      "metadata": {
        "id": "PeFYAL8dd9Lb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Tou5PMxe0S7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}